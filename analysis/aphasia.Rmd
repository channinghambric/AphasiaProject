---
title: "Aphasia  Project"
author: "Channing Hambric"
date: "2024-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries
```{r cars}
library(tidyverse)
library(purrr)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(tidyboot)
library(paletteer)
library(lme4)
library(emmeans)
library(knitr)
library(patchwork)
library(data.table)

```

# Import Raw Data
```{r}
control_fluency<-read.csv("../excel_data/Controls_Fluency.csv")
pwa_fluency<-read.csv("../excel_data/PWA_Fluency.csv")
pwa_dx<-read.csv("../excel_data/3_PWA_BatteryData_NoID.csv")
```


## Data Cleaning
```{r}
#reformat data
control_fluency <- control_fluency %>% select(-X1) %>%
  pivot_longer(cols = starts_with("X"),  
               names_to = "Word",           
               values_to = "Entry") %>%select(-Word) %>%filter(Entry != "") %>% rename(ID=Fluency.Code)

pwa_fluency <- pwa_fluency %>% select(-X1) %>%
  pivot_longer(cols = starts_with("X"),  
               names_to = "Word",           
               values_to = "Entry") %>%select(-Word) %>%filter(Entry != "") 

#Link dx
dx<-pwa_dx %>%select(Fluency.ID,LangDx) %>% rename(ID=Fluency.ID) %>% mutate(ID = as.character(ID))%>%distinct(ID, .keep_all = TRUE)
pwa_fluency<-pwa_fluency %>%
  left_join(dx,by="ID") %>%
  mutate(LangDx = str_to_lower(LangDx)) %>%mutate_all(~str_replace_all(., "\\s+", ""))

#Counting fluency
control_fluency<- control_fluency %>%group_by(ID) %>% mutate(fluency_score=n()) %>%
  ungroup()

pwa_fluency<-pwa_fluency %>%group_by(ID) %>% mutate(fluency_score=n()) %>%
  ungroup()

#How many pwa have > 10 entries?
pwa_fluency_cutoff<- pwa_fluency %>% filter(fluency_score>=10)

#List of ids to be dropped
pwa_fluency_dropped_ids <- pwa_fluency %>%
  filter(fluency_score < 10)

cutoff_id_count <- pwa_fluency_cutoff %>%
  summarise(Unique_IDs = n_distinct(ID)) %>%
  print()

#recoding rec broca's anomic to just anomic
pwa_fluency_cutoff<-pwa_fluency_cutoff%>%
  mutate(LangDx = ifelse(LangDx == "anomic(rec.broca's)", "anomic", LangDx))

#dx distribution of retained ids
pwa_fluency_dist<- pwa_fluency_cutoff %>% group_by(LangDx) %>%
  summarise(Unique_Participants = n_distinct(ID))
pwa_fluency_dist

#Writing cutoff file to csv
#write.csv(pwa_fluency_cutoff,"pwa_fluency_cleaned.csv")
#write.csv(control_fluency,"control_fluency_cleaned.csv")
```
# Import Forager Output
```{r}
#Lexical data
control_lex_results <- read_csv(unz("../forager/output/control/animals_forager_results.zip","lexical_results.csv")) %>% mutate(Group = "Control")
pwa_lex_results=read_csv(unz("../forager/output/pwa/animals_forager_results.zip","lexical_results.csv")) %>% mutate(Group = "PWA")

all_lexical = rbind(control_lex_results,pwa_lex_results)

#Indv desc data
control_indv_results <- read_csv(unz("../forager/output/control/animals_forager_results.zip","individual_descriptive_stats.csv")) %>% mutate(Group = "Control")
pwa_indv_results=read_csv(unz("../forager/output/pwa/animals_forager_results.zip","individual_descriptive_stats.csv")) %>% mutate(Group = "PWA")

all_indv = rbind(control_indv_results,pwa_indv_results) %>% rename("fluency_score"="#_of_Items")%>%separate(Switch_Method, 
           into = c("method", "param1", "param2", "param3"), #separating out parameters
           sep = "_", 
           fill = "right")%>%mutate(full_method=paste(method,param1,param2,param3,sep="_"))

#Models
control_model_results <- read_csv(unz("../forager/output/control/animals_forager_results.zip","model_results.csv")) %>% mutate(Group = "Control")
pwa_model_results=read_csv(unz("../forager/output/pwa/animals_forager_results.zip","model_results.csv")) %>% mutate(Group = "PWA")

all_models = rbind(control_model_results,pwa_model_results)%>%
  separate(Model, 
           into = c("forage", "foraging_type", "method", "param1", "param2", "param3"), #separating out model parameters
           sep = "_", 
           fill = "right") %>%
  mutate(
    forage = factor(forage),
    foraging_type = factor(foraging_type),
    method = factor(method),
    param1 = factor(param1),
    param2 = factor(param2),
    param3 = factor(param3))%>%
  mutate(model_type = fct_recode(foraging_type, 
                                  `pstatic` = "phonologicalstatic", #renaming models
                                  `plocal` = "phonologicaldynamiclocal",
                                  `pglobal` = "phonologicaldynamicglobal",
                                  `pswitch` = "phonologicaldynamicswitch",
                                  `static` = "static", 
                                  `dynamic` = "dynamic", 
                                  `random` = "random")) %>%
  mutate(full_method=paste(method,param1,param2,param3,sep="_"))%>%
  mutate(everything=paste(model_type,full_method,sep="_"))

#Switch designations
control_switch_results <- read_csv(unz("../forager/output/control/animals_forager_results.zip","switch_results.csv")) %>% mutate(Group = "Control")
pwa_switch_results=read_csv(unz("../forager/output/pwa/animals_forager_results.zip","switch_results.csv")) %>% mutate(Group = "PWA")

all_switches = rbind(control_switch_results,pwa_switch_results)%>% separate(Switch_Method, into = c("method", "param1", "param2", "param3"), sep = "_", fill = "right")%>%mutate(full_method=paste(method,param1,param2,param3,sep="_")) 

```

# Lexical Analyses 
```{r}
#import words lengths
control_lengths<-read.csv("control_wordlength.csv") %>%rename("Subject"="ID") %>%mutate(Group="Control")
pwa_lengths<-read.csv("pwa_wordlength.csv")%>%rename("Subject"="ID")%>%mutate(Group="PWA")%>%mutate(Subject=as.factor(Subject))
comb_wordlength <- rbind(control_lengths, pwa_lengths) %>%
  group_by(Subject, Group) %>%
  summarize(avg_length = mean(Phoneme.Count, na.rm = TRUE))
                       
#setting up the df
lexical_analysis_data<-all_indv %>% select(Subject, Group, Semantic_Similarity_mean, Phonological_Similarity_mean, Frequency_Value_mean, 
         fluency_score) %>% na.omit() %>%
  rename("semantic" = "Semantic_Similarity_mean", 
         "phonological" = "Phonological_Similarity_mean",
         "frequency" = "Frequency_Value_mean")%>%
  pivot_longer(names_to = "cue", cols=c(semantic,phonological,frequency)) %>%left_join(comb_wordlength,by=c("Subject","Group"))

#Does fluency score differ by group, avg word length as a covariate (*Tested as intx, no sig diff in models)
fluency_lm = lm(data = lexical_analysis_data, fluency_score ~ Group+avg_length)
summary(fluency_lm)
car::Anova(fluency_lm)


#Does semantic similarity, phonological similarity, and frequency differ across Groups
# Run lm() for each cue and extract results
lexical_results <- lexical_analysis_data  %>%
  group_by(cue) %>%                                   # Group by cue
  nest() %>%                                         # Nest data by cue
 mutate(
    model = map(data, ~ lm(value ~ Group*fluency_score+avg_length, data = .)) # Fit models
  )
#View results
model_summaries <- lexical_results %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(model_summaries)



```

### Lexical Plots
```{r}

#fluency density plot
fluency_plot <- lexical_analysis_data %>%
  ggplot(aes(x = fluency_score, fill = Group, color = Group)) +
  geom_density(alpha = 0.8) +  # Adjust transparency for overlapping densities
  xlim(0, 50) +  
  labs(y = 'Density', x = "Fluency Score", title = "") +
  theme_few() +
  theme(
    aspect.ratio = 1, 
    axis.text.y = element_text(size = 14),
    axis.text.x = element_text(size = 14),
    legend.position = 'right',
    plot.title = element_text(size = 18)  
  ) +
  scale_fill_paletteer_d("lisa::SandroBotticelli") +
  scale_color_paletteer_d("lisa::SandroBotticelli")  # Match fill and line colors


#word length plot
length_plot <- lexical_analysis_data %>%
  ggplot(aes(x = Group, y = avg_length, fill = Group)) +
  geom_boxplot(color = "black") +
  ylim(2,6) +  
  labs(y = 'Average Word Length (# of Phonemes)', x = "Group", title = "") +
  theme_few() +
  theme(
    aspect.ratio = 1, axis.text.y = element_text(size = 14),axis.text.x = element_text(size = 14),
    legend.position = 'right',
    plot.title = element_text(size = 18)  
  ) +
  scale_fill_paletteer_d("lisa::SandroBotticelli") 
fluency_plot+length_plot

#raw values
cue_labels <- c(
  `Semantic\nSimilarity` = "Semantic Similarity",
  `Phonological\nSimilarity` = "Phonological Similarity",
  `Word\nFrequency` = "Word Frequency"
)

lexical_raw_plot <- lexical_analysis_data %>%
  group_by(Group, cue) %>%
  tidyboot_mean(value, nboot = 100, na.rm = TRUE) %>%
  mutate(cue = fct_recode(cue, 
                      `Semantic\nSimilarity` = "semantic", 
                      `Phonological\nSimilarity` = "phonological",
                      `Word\nFrequency` = "frequency"),
         cue = factor(cue, levels = c("Semantic\nSimilarity", 
                                      "Phonological\nSimilarity", 
                                      "Word\nFrequency"))) %>%  # 
  ggplot(aes(x = Group, y = empirical_stat, group = Group, fill = Group)) + 
  facet_wrap(~cue, scales = "free_y", labeller = as_labeller(cue_labels)) + 
  geom_col() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.1) +
  labs(y = NULL, x = "Group") +  
  theme_few() +
  scale_fill_paletteer_d("lisa::SandroBotticelli")  

lexical_raw_plot

#model predicted
plots_lexical <- lexical_results %>% 
  mutate(
    emmip_plot = map(model, ~ 
      emmip(.x, Group ~ fluency_score,  
            cov.reduce = range, CI = TRUE) +
      labs(title = "Group", x = "Fluency score", y = paste("Linear prediction", "(", unique(cue), ")"))+ 
      scale_colour_paletteer_d("lisa::SandroBotticelli") +
      theme_few() + 
      theme(aspect.ratio = 1.5, legend.position = "top")))


plots_lexical$emmip_plot[[1]]+plots_lexical$emmip_plot[[2]]+plots_lexical$emmip_plot[[3]]

```

# Best Models
```{r}

#FOR OVERALL GROUP

#Getting N
just_fluency<-all_indv %>%select(Subject,Group,fluency_score) %>%rename("N"="fluency_score") %>%na.omit() %>%unique()
# Extract random model's nLL for each Subject x Group
random_nLL_lookup <- all_models %>%
  group_by(Subject, Group) %>%
  filter(model_type == "random") %>%
  mutate(random_k = 1) %>% 
  select(Subject, Group, random_k,random_nLLs = Negative_Log_Likelihood_Optimized)


#setting up parameter matching
full_model_data_parameters<-all_models %>%mutate(parameter_link=paste(model_type,method,sep="_"))

#importing parameter counts
parameters<-read.csv("Aphasia_BIC_Parameters.csv") %>%select(parameter_link,k)

bic <- full_model_data_parameters %>%
  left_join(just_fluency, by = c("Subject", "Group")) %>%
  left_join(random_nLL_lookup, by = c("Subject","Group")) %>%  
  left_join(parameters, by="parameter_link") %>%
  mutate(
    optimalBIC = k * log(N) - 2 * (-Negative_Log_Likelihood_Optimized),
    randomBIC = random_k * log(N) - 2 * (-random_nLLs))


#lowest BIC model per participant
lowest_bic = bic %>%
  group_by(Subject) %>%
  slice_min(optimalBIC)

#Looking at distribution
lowest_bic %>%
  ggplot(aes(x = optimalBIC, fill = Group)) + facet_wrap(~Group)+
  geom_density(fill = "#69b3a2", color = "#e9ecef", alpha = 0.8)


#median delta BIC
median_delta_bic = bic %>% 
  mutate(deltaBIC  = randomBIC - optimalBIC) %>%
  group_by(Group,everything) %>%
  summarise(medianBIC = median(deltaBIC)) %>%
  arrange(desc(medianBIC)) %>%
  slice(1) %>% 
  ungroup() 
kable(median_delta_bic)


#BY PARTICIPANT BEST MODELS 
subject_median_delta_bic = bic %>% 
  mutate(deltaBIC  = randomBIC - optimalBIC) %>%
  group_by(Subject,everything) %>%
  summarise(medianBIC = median(deltaBIC)) %>%
  arrange(desc(medianBIC)) %>%
  slice(1) %>% 
  ungroup() 


```

### Best model distribution
```{r}
  
best_models_p%>%
    group_by(Group,model_type) %>%
    count() %>%
    ggplot(aes(x = Group, y = n, group = model_type, fill = model_type)) +
    geom_col() +
    labs(y = "number of participants", x = "", fill = "best model type") +  
    theme_few() +
    scale_fill_calc() +
    theme(
    aspect.ratio = 1, 
    legend.position = 'right',  # Position legend at the top
    axis.text.x = element_text(angle = 45, hjust = 1, face = "bold")) +
  scale_fill_paletteer_d("rcartocolor::Antique")


```


# Beta Analyses
```{r}

#BY GROUP
#Filtering full data to only include best model for each domain type
best_models <- all_models %>%
  filter(paste(Group,everything) %in% paste(median_delta_bic$Group, median_delta_bic$everything)) %>%
  group_by(Group)

#Separating out betas
beta_values  = best_models  %>%
  pivot_longer(names_to = "beta", cols = c(Beta_Frequency, Beta_Phonological, Beta_Semantic))


# Run lm() for each beta type and extract results
beta_results <- beta_values  %>%
  group_by(beta) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group, data = .)) 
  )

#View results
beta_summaries <- beta_results %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(beta_summaries)


#By PARTICIPANT BEST MODELS
best_models_p <- all_models %>%
  filter(paste(Subject,everything) %in% paste(subject_median_delta_bic$Subject, subject_median_delta_bic$everything))

#Separating out betas
beta_values_p  = best_models_p  %>%
  pivot_longer(names_to = "beta", cols = c(Beta_Frequency, Beta_Phonological, Beta_Semantic))

# Run lm() for each beta type and extract results
beta_results_p <- beta_values_p  %>%
  group_by(beta) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group, data = .)) 
  )

#View results
beta_summaries_p <- beta_results_p %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(beta_summaries_p)

```


### Beta plot
```{r}

#BY GROUP BEST MODEL
beta_values %>%
  group_by(Group,beta) %>%
  tidyboot_mean(value, nboot = 1000, na.rm = TRUE) %>%
  separate(beta, into = c("b", "beta")) %>%
  mutate(
    beta = tolower(beta),
    beta = fct_recode(beta, 
                      `Semantic\nSimilarity` = "semantic", 
                      `Phonological\nSimilarity` = "phonological",
                      `Word\nFrequency` = "frequency")
  ) %>%
  mutate(beta = fct_relevel(beta, "Semantic\nSimilarity", "Phonological\nSimilarity", "Word\nFrequency")) %>%
  ggplot(aes(x = beta, y = empirical_stat, group = Group, fill = Group)) +
  geom_bar(stat = 'identity', position = "dodge",color="black") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = position_dodge(0.9)) +
  labs(y = bquote(beta ~ "(Parameter Salience)"), x = "") +
  theme_few() +
  theme() +
  scale_fill_paletteer_d("lisa::SandroBotticelli")

```

# Clustering Analyses
```{r}

#BY GROUP
#filtering to only include best method from above
best_clusters <- all_indv%>%
  filter(paste(Group, full_method) %in% paste(best_models$Group, best_models$full_method)) %>%select(-fluency_score)%>% #this is full of NAs & produces mismatch .x/.y error, so dropping
  group_by(Group) %>%left_join(lexical_analysis_data%>%select(Subject,Group,fluency_score)%>%unique(), by = c("Subject", "Group")) #adding fluency_score back in

#Participant best models
best_clusters_p <- all_indv%>%
  filter(paste(Subject, full_method) %in% paste(best_models_p$Subject, best_models_p$full_method)) %>%select(-fluency_score)%>% #this is full of NAs & produces mismatch .x/.y error, so dropping
  group_by(Group) %>%left_join(lexical_analysis_data%>%select(Subject,Group,fluency_score)%>%unique(), by = c("Subject", "Group")) #adding fluency_score back in

#When using the best method for each Group, does avg cluster mean differ by Group x domain
#number of items produced as a covariate (intx does not sig add to model)
cluster_size_lm = lm(data = best_clusters, Cluster_Size_mean ~ Group + fluency_score)
summary(cluster_size_lm)
car::Anova(cluster_size_lm)

#When using the best method for each Group, does avg numb switches differ by Group x domain
switch_numb_lm = lm(data = best_clusters, Number_of_Switches ~Group + fluency_score)
summary(switch_numb_lm)
car::Anova(switch_numb_lm)


#Replicating prev analyses where they don't include fluency score as covariate
cluster_size_lm_no_fs = lm(data = best_clusters, Cluster_Size_mean ~ Group)
summary(cluster_size_lm_no_fs)
car::Anova(cluster_size_lm_no_fs)

#When using the best method for each Group, does avg numb switches differ by Group x domain
switch_numb_lm_no_fs = lm(data = best_clusters, Number_of_Switches ~Group)
summary(switch_numb_lm_no_fs)
car::Anova(switch_numb_lm_no_fs)

```

### Cluster Plots
```{r}
#cluster size
clusters<-emmip(cluster_size_lm,Group ~ fluency_score, cov.reduce = range, CI = TRUE) + 
  guides(color = guide_legend(title = NULL)) +
  theme_few() +
  scale_colour_paletteer_d("lisa::SandroBotticelli") +
  labs(
    x = "Fluency score",
    y = "Predicted cluster size") +theme(aspect.ratio=.75,legend.position="right")

#number of switches
switches<-emmip(switch_numb_lm, Group ~ fluency_score, cov.reduce = range,CI=TRUE)+ 
      guides(color = guide_legend(title = NULL)) +  
      theme_few() + scale_colour_paletteer_d("lisa::SandroBotticelli") +
  labs(
    x = "Fluency score",
    y = "Predicted number of switches")+theme(aspect.ratio=.75)

clusters+switches+ plot_layout(ncol = 1)
```

# Within Cluster vs Switch Trial Lexical Metrics
```{r}
#By participant best models
#filtering to only include best method for each domain
best_switches <- all_switches %>% 
  filter(paste(Group,full_method) %in% paste(best_models$Group, best_models$full_method)) %>%
  mutate(Switch_Value = if_else(Switch_Value == 2, 1, Switch_Value))

#Calculate item_no consistently in all_lexical to use in joining
items_lexical_data <- all_lexical %>%
  group_by(Subject, Group) %>%
  mutate(row_id = row_number() - 1)  # Adjust row_id to start at 0 to account for first item

#this code drops the first item from each list, and if switch_value = 1, marked as a switch trial, if switch_value=1, marked as within cluster transition, also labels clusters and switches by ordinal position
#new first item only marked as within cluster if original switch_value==0
switches_recoded <- best_switches %>%
  group_by(Subject,Group) %>%
  mutate(
    Cluster_Number = cumsum(Switch_Value == 1),
    Switch = case_when(
      row_number() == 1 ~ "Within Cluster Transition",  # Changed to "Within Cluster Transition"
      Cluster_Number != lag(Cluster_Number) ~ "Switch Trial", # Adding cluster number
      TRUE ~ "Within Cluster Transition"
    )
  ) %>%
  mutate(Switch_Number = case_when(
    Switch == "Switch Trial" ~ Cluster_Number - 1,  # Adding switch number
    TRUE ~ NA_real_
  )) %>%
  group_by(Subject,Group) %>%
  mutate(row_id = row_number() - 1) %>%  # Adjusting row_id to start at 0 to be able to drop first item
  left_join(items_lexical_data, by = c("Subject","Group", "Fluency_Item","row_id")) %>%
  rename(
    "semantic" = "Semantic_Similarity",
    "phonological" = "Phonological_Similarity",
    "frequency" = "Frequency_Value"
  ) %>% filter(row_id!=0)%>% #dropping first word of each list
  pivot_longer(names_to = "cue", cols = c(semantic, phonological, frequency))

write.csv(switches_recoded,"switches.csv")
                                                                                       
                                                                                       
## does cluster/switch designation predict semantic similarity, phonological similarity, and frequency for each Group 
# Run lmer() for each cue type and extract results

switch_results <- switches_recoded  %>%
  group_by(cue) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group*Switch, data = .)) 
  )

#View results
#large so calling indp
print("Semantic")
switch_results$model[[1]] %>% summary()
print("phonological")
switch_results$model[[2]] %>% summary()
print("Frequency")
switch_results$model[[3]] %>% summary()


#Looking at ordinal effects of cluster and switch numbers
switches_recoded_ordinal<-switches_recoded %>% mutate(Switch_Number=as.numeric(Switch_Number),Cluster_Number=as.numeric(Cluster_Number))


#Cluster Number
cluster_results <- switches_recoded_ordinal %>% filter(Switch=="Within Cluster Transition")%>%
  group_by(cue) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group*Cluster_Number, data = .)) 
  )

#View results
#large so calling indp
print("Semantic")
cluster_results$model[[1]] %>% summary()
print("phonological")
cluster_results$model[[2]] %>% summary()
print("Frequency")
cluster_results$model[[3]] %>% summary()


#Switch Number
switchpoint_results <- switches_recoded_ordinal %>% filter(Switch=="Switch Trial")%>%
  group_by(cue) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group*Switch_Number, data = .)) 
  )

#View results
#large so calling indp
print("Semantic")
switchpoint_results$model[[1]] %>% summary()
print("phonological")
switchpoint_results$model[[2]] %>% summary()
print("Frequency")
switchpoint_results$model[[3]] %>% summary()


```

### WIAC Plots
```{r}

# Takes a long time w dyplr, converting to data.table
setDT(switches_recoded)

# Configure plots
plot_configs <- list(
  list(cue = "semantic", value_col = "value", y_label = "Semantic similarity", plot_title = ""),
  list(cue = "phonological", value_col = "value", y_label = "Phonological similarity", plot_title = ""),
  list(cue = "frequency", value_col = "value", y_label = "Word frequency", plot_title = "")
)

# Generate plots
wiac_cluster_plots <- map(plot_configs, ~ {
  data_subset <- switches_recoded[cue == .x$cue]
  
  data_subset[, Switch := fifelse(.x$cue == "frequency" & is.na(Switch), 
                                  "Cluster Transition", Switch)]
  
 data_subset[, Switch := factor(Switch, levels = c("Within Cluster Transition", "Switch Trial"))]
  
  # Convert back to tibble for tidyboot
  data_subset_tibble <- as_tibble(data_subset)
  
  boot_summary <- data_subset_tibble %>%
    group_by(Switch, Group) %>%
    tidyboot_mean(!!sym(.x$value_col), nboot = 1000, na.rm = TRUE)

  # Plot 
  ggplot(boot_summary, aes(fill = Switch, y = empirical_stat, x = Group)) +
    geom_bar(stat = "identity", position = "dodge",color="black") +
   geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.05, position = position_dodge(0.9)) +
   labs(
      title = .x$plot_title,
      y = .x$y_label,
      x = "",
      color = ""
  ) +
    theme_few() +
    theme(
      aspect.ratio = 1.5,
      legend.position = "top"
    ) +
   scale_fill_manual(values = c("darkorange4", "coral3"))
})

wiac_cluster_plots[[1]]+
wiac_cluster_plots[[2]]+ 
wiac_cluster_plots[[3]]


# Ordinal Plots
plots_cluster_num <- cluster_results %>% 
  mutate(
    emmip_plot = map(model, ~ 
      emmip(.x, Group ~ Cluster_Number,  
            cov.reduce = range, CI = TRUE) +
      labs(title = "Group", x = "Cluster Number", y = paste("Linear prediction", "(", unique(cue), ")")) +
      theme_few() + 
      theme(aspect.ratio = 1.5, legend.position = "top")))


plots_cluster_num$emmip_plot[[1]]+plots_cluster_num$emmip_plot[[2]]+plots_cluster_num$emmip_plot[[3]]



# Ordinal Plots
plots_switch_num <- switchpoint_results %>% 
  mutate(
    emmip_plot = map(model, ~ 
      emmip(.x, Group ~ Switch_Number,  
            cov.reduce = range, CI = TRUE) +
      labs(title = "Group", x = "Switch Number", y = paste("Linear prediction", "(", unique(cue), ")"))+
      theme_few() + 
      theme(aspect.ratio = 1.5, legend.position = "top")))


plots_switch_num$emmip_plot[[1]]+plots_switch_num$emmip_plot[[2]]+plots_switch_num$emmip_plot[[3]]

```

# Exploring Battery Measures for PWA
```{r}

#Battery descriptions & means from Mirman et al. (2015)

#Philadelphia Naming Test (PNT)
#A 175-item single-word picture naming test using drawings, target words cover a relatively wide range of word length, word
#frequency, and semantic category. The pictures are all familiar objects with high name agreement (97%)
#Overall percent correct (M=63.3, SD=29.1, Range=1.1–97.7)
#Percent of semantic errors (M=5.4, SD=3.9, Range=1.1–18.3)
#percent of phonological errors (M=13.2, SD=13.3, Range=0–49.1).

#Philadelphia Repetition Test
#A word repetition test using the same set of 175 targets as the Philadelphia Naming Test. 
#Performance is measured by percent correct: M=85.9, SD=14.1, Range=39–100.

#Camels and Cactus Test (camcac.)
#Test of non-verbal semantic processing in which a pictured item must be matched to the closest associate 
#among a set of four pictured choices (e.g., wine matched to: grapes, cherry, strawberry,orange). 
#Performance is measured by percent correct of 64 trials: M=75.2, SD=15.0, Range=25–95.

#Synonymy triplets (syntot.)
#Test of verbal semantic processing in which participants must decide which two of three words are most similar in meaning. Half
#the trials involve nouns (e.g., violin, fiddle, clarinet), the other half
#verbs (e.g., to repair, to design, to fix). Performance is measured by percent correct of 30 trials: M=79.1, SD=16.9, Range=33–100.


#Nonword Repetition Test (nonwdrep..)
#Pre-recorded nonword targets derived from PNT,  target words were presented to participants for repetition. 
#Performance is measured by percent correct of 60 trials: M=47.3, SD=25.8, Range=0–98.

#set up dfs

battery <- pwa_dx %>%
  rename(Subject = Fluency.ID) %>% filter(Subject %in% all_indv$Subject)%>%
  select(Subject, TCorrPNT, TrueSem, TrueFor, TrueMix, TrueNonW, prt, syntot., camcac.,nonwdrep..) %>%
  mutate(
    Subject = as.factor(Subject),
    across(-Subject, ~ as.numeric(as.character(.))) 
  ) %>%
  na.omit()

#NOTE 9 IDs have two entries - dropping these  
participant_drop_list <- battery %>%
  count(Subject) %>%  
  filter(n > 1) %>%  
  select(Subject)

#creating composite measures
battery<-battery %>%mutate(semantic_output=TrueSem+TrueMix, phon_output=TrueNonW+TrueFor) %>%filter(!Subject %in% participant_drop_list$Subject) #drop duplicates
write.csv(battery,"battery.csv")


#LOOKING AT CORRELATIONS
library(corrplot)
corrplot(cor(battery%>%select(-Subject,-TrueSem,-TrueMix,-TrueFor,-TrueNonW)))

#getting word lengths
pwa_avg_length<-pwa_lengths %>%  group_by(Subject) %>%
  summarize(avg_length = mean(Phoneme.Count, na.rm = TRUE))


lexical_battery_wide<-all_indv %>% select(Subject, Group, Semantic_Similarity_mean, Phonological_Similarity_mean, Frequency_Value_mean, 
         fluency_score) %>% na.omit() %>%filter(Group=="PWA") %>%left_join(battery,by="Subject")  %>% na.omit() %>%
  rename("semantic" = "Semantic_Similarity_mean", 
         "phonological" = "Phonological_Similarity_mean",
         "frequency" = "Frequency_Value_mean") %>% unique()%>%filter(!Subject %in% participant_drop_list$Subject)
write.csv(lexical_battery_wide,"lexical_metrics_battery.csv") # save wide version to csv


#Lexical df
lexical_battery<-all_indv %>% select(Subject, Group, Semantic_Similarity_mean, Phonological_Similarity_mean, Frequency_Value_mean, 
         fluency_score) %>% na.omit() %>%filter(Group=="PWA") %>%left_join(battery,by="Subject") %>%left_join(pwa_avg_length,by="Subject") %>% na.omit() %>%
  rename("semantic" = "Semantic_Similarity_mean", 
         "phonological" = "Phonological_Similarity_mean",
         "frequency" = "Frequency_Value_mean")%>%
  pivot_longer(names_to = "cue", cols=c(semantic,phonological,frequency)) %>%filter(!Subject %in% participant_drop_list$Subject)



#Clustering df
##Note this is using participant best model
clusters_battery<-best_clusters_p %>%filter(Group=="PWA") %>%left_join(battery,by="Subject")%>%filter(!Subject %in% participant_drop_list$Subject)
write.csv(clusters_battery,"clustering_metrics_battery.csv") # save to csv


```
## Lexical Analyses
```{r}

fluency_battery_lm = lm(data = lexical_battery, fluency_score ~ TCorrPNT+semantic_output+phon_output+prt+syntot.+camcac.+nonwdrep..+avg_length)
summary(fluency_battery_lm)
car::Anova(fluency_battery_lm)


#Does semantic similarity, phonological similarity, and frequency differ across measures
# Run lm() for each cue and extract results
lexical_battery_results <- lexical_battery  %>%
  group_by(cue) %>%                                   # Group by cue
  nest() %>%                                         # Nest data by cue
 mutate(
    model = map(data, ~ lm(value ~ TCorrPNT+semantic_output+phon_output+prt+syntot.+camcac.+nonwdrep..+avg_length, data = .)) # Fit models
  )
#View results
model_battery_summaries <- lexical_battery_results %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(model_battery_summaries)
```

### Fluency Plots

```{r}
emm_results <- list(
  TCorrPNT = emmeans(fluency_battery_lm, ~ TCorrPNT, cov.reduce = range),
  semantic_output  = emmeans(fluency_battery_lm, ~ semantic_output, cov.reduce = range),
  phon_output  = emmeans(fluency_battery_lm, ~ phon_output, cov.reduce = range),
  prt      = emmeans(fluency_battery_lm, ~ prt, cov.reduce = range),
  syntot.  = emmeans(fluency_battery_lm, ~ syntot., cov.reduce = range),
  camcac.  = emmeans(fluency_battery_lm, ~ camcac., cov.reduce = range),
  nonwdrep..  = emmeans(fluency_battery_lm, ~ nonwdrep.., cov.reduce = range)

)

#plot sig predictors
#PNT
plot_pnt <- emmip(emm_results$TCorrPNT, ~ TCorrPNT, CIs = TRUE) +
  labs(title = "",
       x = "PNT Score (NCorr)", 
       y = "Estimated Fluency Score") +
  theme_few() +
  theme(aspect.ratio = 1.5, legend.position = "top")
plot_pnt

#Sem Errors (PNT)
plot_sem <- emmip(emm_results$semantic_output, ~ semantic_output, CIs = TRUE) +
  labs(title = "",
       x = "Semantic Output Errors in PNT", 
       y = "Estimated Fluency Score")  +
  theme_few() +
  theme(aspect.ratio = 1.5, legend.position = "top")
plot_sem

#Phon Errors (PNT)
plot_phon <- emmip(emm_results$phon_output, ~ phon_output, CIs = TRUE) +
  labs(title = "",
       x = "Phon Output Errors in PNT", 
       y = "Estimated Fluency Score")  +
  theme_few() +
  theme(aspect.ratio = 1.5, legend.position = "top")
plot_phon

#Synonymy (syntot)
plot_syntot <- emmip(emm_results$syntot., ~ syntot., CIs = TRUE) +
  labs(title = "",
       x = "syntot score", 
       y = "Estimated Fluency Score")  +
  theme_few() +
  theme(aspect.ratio = 1.5, legend.position = "top")
plot_syntot


#Nonword repetition
plot_nwr <- emmip(emm_results$nonwdrep.., ~ nonwdrep.., CIs = TRUE) +
  labs(title = "",
       x = "nonword repetition score", 
       y = "Estimated Fluency Score")  +
  theme_few() +
  theme(aspect.ratio = 1.5, legend.position = "top")
plot_nwr

#the better the performance on the PNT & syntot, the higher the fluency score
#But those with higher frequency scores also produced more form errors in PNT
```
### Lexical plots

```{r}

#just significant effects/predictors
predictors_phon <- c( "syntot.", "camcac.")

plots_lexical_phon <- lexical_battery_results %>% filter(cue=="phonological")%>%
  mutate(
    emmip_plot = map2(model, cue, ~ {
      predictor_plots <- map(predictors_phon, function(pred) {
        emm <- emmeans(.x, as.formula(paste("~", pred)), cov.reduce = range)
        emmip(emm, as.formula(paste("~", pred)), CIs = TRUE) +
          labs(title = paste("",  ""),
               x = pred, 
               y = "Estimated Phonological Similarity")  +
          theme_few() +
          theme(aspect.ratio = 1.5, legend.position = "top")
      })
      patchwork::wrap_plots(predictor_plots)  # Combine
    })
  )

plots_lexical_phon$emmip_plot[[1]]

#for verbal semantic processing/comprehension tasks (syntot), high performance assc with lower inter-item phonological similarity in SFT  (less interference/more control over semantic processing)
#for NON verbal semantic association/processing tasks (camcac), high score assc with high avg inter-item phonological similarity in SFT


predictors_freq <- c( "TCorrPNT", "phon_output","avg_length")

plots_lexical_freq <- lexical_battery_results %>% filter(cue=="frequency")%>%
  mutate(
    emmip_plot = map2(model, cue, ~ {
      predictor_plots <- map(predictors_freq, function(pred) {
        emm <- emmeans(.x, as.formula(paste("~", pred)), cov.reduce = range)
        emmip(emm, as.formula(paste("~", pred)), CIs = TRUE) +
          labs(title = paste("",  ""),
               x = pred, 
               y = "Estimated Avg Word Frequency")  +
          theme_few() +
          theme(aspect.ratio = 1.5, legend.position = "top")
      })
      patchwork::wrap_plots(predictor_plots)  # Combine
    })
  )

plots_lexical_freq$emmip_plot[[1]]

```
## Cluster Analyses
```{r}
#When using the best method, does avg cluster mean differ by battery measures
cluster_battery_lm = lm(data = clusters_battery, Cluster_Size_mean ~ TCorrPNT+semantic_output+phon_output+prt+syntot.+camcac.)
summary(cluster_battery_lm)
car::Anova(cluster_battery_lm)
#all ns

switch_battery_lm = lm(data = clusters_battery, Number_of_Switches ~ TCorrPNT+semantic_output+phon_output+prt+syntot.+camcac.)
summary(switch_battery_lm)
car::Anova(switch_battery_lm)
# sig predictor of syntot

#All ns

```


### Cluster Plots

```{r}
emm_results_switch <- list(
  syntot. = emmeans(switch_battery_lm, ~ syntot., cov.reduce = range)
)

#plot sig predictors
switch_syntot <- emmip(emm_results_switch$syntot., ~ syntot., CIs = TRUE) +
  labs(title = "",
       x = "syntot.", 
       y = "Number of Switches") +
  theme_few() +
  theme(aspect.ratio = 1.5, legend.position = "top")
switch_syntot
```






```{r}
```{r}