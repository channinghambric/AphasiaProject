---
title: "Aphasia  Project"
author: "Channing Hambric"
date: "2024-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# SET UP
## Libraries
```{r cars}
library(tidyverse)
library(purrr)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(tidyboot)
library(paletteer)
library(lme4)
library(emmeans)
library(knitr)
library(patchwork)
library(data.table)
library(dplyr)
library(gt)
library(glmnet)
```

## Import Raw Data
```{r}
#control_fluency<-read.csv("../excel_data/Controls_Fluency.csv")
#pwa_fluency<-read.csv("../excel_data/PWA_Fluency.csv")
pwa_dx<-read.csv("Battery_Clean.csv")
```


## Data Cleaning & Organization
```{r}
#reformat data
#control_fluency <- control_fluency %>% select(-X1) %>%
#  pivot_longer(cols = starts_with("X"),  
#               names_to = "Word",           
#               values_to = "Entry") %>% select(-Word) %>% filter(Entry != "") %>% rename(ID=Fluency.Code)

#pwa_fluency <- pwa_fluency %>% select(-X1) %>%
#  pivot_longer(cols = starts_with("X"),  
#               names_to = "Word",           
#               values_to = "Entry") %>%select(-Word) %>%filter(Entry != "") 

#Link dx
dx<-pwa_dx %>%select(Subject,SubType,WABAQ) %>% rename(ID=Subject) %>% mutate(ID = as.character(ID))%>%distinct(ID, .keep_all = TRUE)
#pwa_fluency<-pwa_fluency %>%
#  left_join(dx,by="ID") %>%
#  mutate(LangDx = str_to_lower(LangDx)) %>%mutate_all(~str_replace_all(., "\\s+", ""))

#Counting fluency
#control_fluency<- control_fluency %>%group_by(ID) %>% mutate(fluency_score=n()) %>%
#  ungroup()

#pwa_fluency<-pwa_fluency %>%group_by(ID) %>% mutate(fluency_score=n()) %>%
#  ungroup()

#How many pwa have > 10 entries?
#pwa_fluency_cutoff<- pwa_fluency %>% filter(fluency_score>=10)

#List of ids to be dropped
#pwa_fluency_dropped_ids <- pwa_fluency %>%
#  filter(fluency_score < 10)

#cutoff_id_count <- pwa_fluency_cutoff %>%
#  summarise(Unique_IDs = n_distinct(ID)) %>%
#  print()

#recoding rec broca's anomic to just anomic
#pwa_fluency_cutoff<-pwa_fluency_cutoff%>%
#  mutate(LangDx = ifelse(LangDx == "anomic(rec.broca's)", "anomic", LangDx))

#dx distribution of retained ids
#pwa_fluency_dist<- pwa_fluency_cutoff %>% group_by(LangDx) %>%
#  summarise(Unique_Participants = n_distinct(ID))
#pwa_fluency_dist

#Writing cutoff file to csv
#write.csv(pwa_fluency_cutoff,"pwa_fluency_cleaned.csv")
#write.csv(control_fluency,"control_fluency_cleaned.csv")
```

## Import Forager Output
```{r}
#Lexical data
control_lex_results <- read_csv(unz("../forager/output/control/animals_forager_results.zip","lexical_results.csv")) %>% mutate(Group = "Control")
pwa_lex_results=read_csv(unz("../forager/output/pwa/animals_forager_results.zip","lexical_results.csv")) %>% mutate(Group = "PWA")

all_lexical = rbind(control_lex_results,pwa_lex_results)

#Indv desc data
control_indv_results <- read_csv(unz("../forager/output/control/animals_forager_results.zip","individual_descriptive_stats.csv")) %>% mutate(Group = "Control")
pwa_indv_results=read_csv(unz("../forager/output/pwa/animals_forager_results.zip","individual_descriptive_stats.csv")) %>% mutate(Group = "PWA")

all_indv = rbind(control_indv_results,pwa_indv_results) %>% rename("fluency_score"="#_of_Items")%>%separate(Switch_Method, 
           into = c("method", "param1", "param2", "param3"), #separating out parameters
           sep = "_", 
           fill = "right")%>%mutate(full_method=paste(method,param1,param2,param3,sep="_"))

#Models
control_model_results <- read_csv(unz("../forager/output/control/animals_forager_results.zip","model_results.csv")) %>% mutate(Group = "Control")
pwa_model_results=read_csv(unz("../forager/output/pwa/animals_forager_results.zip","model_results.csv")) %>% mutate(Group = "PWA")

all_models = rbind(control_model_results,pwa_model_results)%>%
  separate(Model, 
           into = c("forage", "foraging_type", "method", "param1", "param2", "param3"), #separating out model parameters
           sep = "_", 
           fill = "right") %>%
  mutate(
    forage = factor(forage),
    foraging_type = factor(foraging_type),
    method = factor(method),
    param1 = factor(param1),
    param2 = factor(param2),
    param3 = factor(param3))%>%
  mutate(model_type = fct_recode(foraging_type, 
                                  `pstatic` = "phonologicalstatic", #renaming models
                                  `plocal` = "phonologicaldynamiclocal",
                                  `pglobal` = "phonologicaldynamicglobal",
                                  `pswitch` = "phonologicaldynamicswitch",
                                  `static` = "static", 
                                  `dynamic` = "dynamic", 
                                  `random` = "random")) %>%
  mutate(full_method=paste(method,param1,param2,param3,sep="_"))%>%
  mutate(everything=paste(model_type,full_method,sep="_"))

#Switch designations
control_switch_results <- read_csv(unz("../forager/output/control/animals_forager_results.zip","switch_results.csv")) %>% mutate(Group = "Control")
pwa_switch_results=read_csv(unz("../forager/output/pwa/animals_forager_results.zip","switch_results.csv")) %>% mutate(Group = "PWA")

all_switches = rbind(control_switch_results,pwa_switch_results)%>% separate(Switch_Method, into = c("method", "param1", "param2", "param3"), sep = "_", fill = "right")%>%mutate(full_method=paste(method,param1,param2,param3,sep="_")) 

```
############################################################################

# LEXICAL ANALYSES
```{r}
#import words lengths
#to get word lengths, cleaned fluency lists were run through word_length.py
control_lengths<-read.csv("control_wordlength.csv") %>%mutate(Group="Control")
pwa_lengths<-read.csv("pwa_wordlength.csv")%>%mutate(Group="PWA")%>%mutate(Subject=as.factor(Subject))
comb_wordlength <- rbind(control_lengths, pwa_lengths) %>%
  group_by(Subject, Group) %>%
  summarize(avg_length = mean(Phoneme.Count, na.rm = TRUE))
                       
#setting up the dfs
lexical_analysis_data_fluency<-all_indv %>% select(Subject, Group, Semantic_Similarity_mean, Phonological_Similarity_mean, Frequency_Value_mean, 
         fluency_score) %>% na.omit() %>%
  rename("semantic" = "Semantic_Similarity_mean", 
         "phonological" = "Phonological_Similarity_mean",
         "frequency" = "Frequency_Value_mean") %>%left_join(comb_wordlength,by=c("Subject","Group"))

lexical_analysis_data<-all_indv %>% select(Subject, Group, Semantic_Similarity_mean, Phonological_Similarity_mean, Frequency_Value_mean, 
         fluency_score) %>% na.omit() %>%
  rename("semantic" = "Semantic_Similarity_mean", 
         "phonological" = "Phonological_Similarity_mean",
         "frequency" = "Frequency_Value_mean")%>%
  pivot_longer(names_to = "cue", cols=c(semantic,phonological,frequency)) %>%left_join(comb_wordlength,by=c("Subject","Group"))

#Does fluency score differ by group, avg word length as a covariate (*Tested as intx, no sig diff in models)
fluency_lm = lm(data = lexical_analysis_data_fluency, fluency_score ~ Group+avg_length)
summary(fluency_lm)
tbl_df <- broom::tidy(fluency_lm, effects = "fixed", conf.int = TRUE) %>%
  transmute(
    Term = term,
    Estimate = round(estimate, 3),
    `CI_low` = round(conf.low, 3),
    `CI_high` = round(conf.high, 3),
    t_value = round(statistic, 3),
    SE = round(std.error, 3),
    p_value = signif(p.value, 3)
  )
#write.csv(tbl_df, "fluency_model_results.csv", row.names = FALSE)

#Does semantic similarity, phonological similarity, and frequency differ across Groups
# Run lm() for each cue and extract results
lexical_results <- lexical_analysis_data  %>%
  group_by(cue) %>%                                   # Group by cue
  nest() %>%                                         # Nest data by cue
 mutate(
    model = map(data, ~ lm(value ~ Group*fluency_score+avg_length, data = .)) # Fit models
  )
#View results
model_summaries <- lexical_results %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(model_summaries)

tbl_df <- lexical_results %>%
  mutate(tidy_model = map(model, ~ broom::tidy(.x, conf.int = TRUE))) %>%
  unnest(tidy_model) %>%
 transmute(
    Cue = cue,
    Term = term,
    Estimate = round(estimate, 3),
    CI_low = round(conf.low, 3),
    CI_high = round(conf.high, 3),
    t_value = round(statistic, 3),
    SE = round(std.error, 3),
    p_value = signif(p.value, 3)
  )

# Write to CSV
#write.csv(tbl_df, "lexical_model_results.csv", row.names = FALSE)

```

## Lexical Plots
```{r}

#fluency plot
fluency_plot <- lexical_analysis_data_fluency %>%
  ggplot(aes(x = Group, 
             y = fluency_score, 
             fill = Group, 
             color = Group)) +
  geom_violin(trim = FALSE, alpha = 0.95,color="black") +
  ylim(0, 50) +  
  labs(y = "Fluency Score", x = "", title = "Fluency Score") +
  theme_few() +
  theme(
    aspect.ratio = 1, 
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 12),
    legend.position = "none",   # redundant with x-axis
    plot.title = element_text(size = 14, hjust = 0.5)  
  ) +
  scale_fill_paletteer_d("MoMAColors::Koons") +
  scale_color_paletteer_d("MoMAColors::Koons")


#word length plot
length_plot <- lexical_analysis_data_fluency %>%
  ggplot(aes(x = Group, y = avg_length, fill = Group)) +
  geom_boxplot(color = "black") +
  ylim(2,6) +  
  labs(y = 'Average Word Length (# of Phonemes)', x = "Group", title = "Word Length") +
  theme_few() +
  theme(
    aspect.ratio = 1, axis.text.y = element_text(size = 12),axis.text.x = element_text(size = 12),
    legend.position = 'right',
    plot.title = element_text(size = 14, hjust = 0.5) 
  ) +
  scale_fill_paletteer_d("MoMAColors::Koons") 
fluency_plot+length_plot

#raw values
cue_labels <- c(
  `Semantic\nSimilarity` = "Semantic Similarity",
  `Phonological\nSimilarity` = "Phonological Similarity",
  `Word\nFrequency` = "Word Frequency"
)

lexical_raw_plot <- lexical_analysis_data %>%
  group_by(Group, cue) %>%
  tidyboot_mean(value, nboot = 100, na.rm = TRUE) %>%
  mutate(cue = fct_recode(cue, 
                      `Semantic\nSimilarity` = "semantic", 
                      `Phonological\nSimilarity` = "phonological",
                      `Word\nFrequency` = "frequency"),
         cue = factor(cue, levels = c("Semantic\nSimilarity", 
                                      "Phonological\nSimilarity", 
                                      "Word\nFrequency"))) %>%  # 
  ggplot(aes(x = Group, y = empirical_stat, group = Group, fill = Group)) + 
  facet_wrap(~cue, scales = "free_y", labeller = as_labeller(cue_labels)) + 
  geom_col() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.1) +
  labs(y = NULL, x = "Group") +  
  theme_few() +
  scale_fill_paletteer_d("MoMAColors::Koons")  

lexical_raw_plot

#model predicted
plots_lexical <- lexical_results %>% 
  mutate(
    emmip_plot = map(model, ~ 
      emmip(.x, Group ~ fluency_score,  
            cov.reduce = range, CI = TRUE) +
      labs(title = "Group", x = "Fluency score", y = paste("Linear prediction", "(", unique(cue), ")"))+ 
      scale_colour_paletteer_d("MoMAColors::Koons") +
      theme_few() + 
      theme(aspect.ratio = 1.5, legend.position = "top")))


plots_lexical$emmip_plot[[1]]+plots_lexical$emmip_plot[[2]]+plots_lexical$emmip_plot[[3]]


#raw values, looking at phon group x fluency score effect
#looking at frequency
ggplot(
  data = lexical_analysis_data %>% filter(cue == "phonological"),
  aes(x = value, y = fluency_score, group = Group, color = Group)
) + 
  theme_few() +
  geom_smooth(method = "lm", se = TRUE, size = 1) +
  geom_point(size = 2, alpha = 0.7) +   # scatter on top
  theme(
    legend.position = "top",
    aspect.ratio = 1,  
    axis.text.x = element_text(hjust = 1, face = "bold")
  ) +
  labs(y = "Fluency Score", x = "Phonological Similarity")+ylim(0,30)+scale_colour_paletteer_d("MoMAColors::Koons")

```
############################################################################

# BEST-FIT MODELS
```{r}
#FOR OVERALL GROUP

#Getting N
just_fluency<-all_indv %>%select(Subject,Group,fluency_score) %>%rename("N"="fluency_score") %>%na.omit() %>%unique()
# Extract random model's nLL for each Subject x Group
random_nLL_lookup <- all_models %>%
  group_by(Subject, Group) %>%
  filter(model_type == "random") %>%
  mutate(random_k = 1) %>% 
  select(Subject, Group, random_k,random_nLLs = Negative_Log_Likelihood_Optimized)


#setting up parameter matching
full_model_data_parameters<-all_models %>%mutate(parameter_link=paste(model_type,method,sep="_"))

#importing parameter counts
parameters<-read.csv("BICparams_noswitch.csv") %>%select(parameter_link,k)

bic <- full_model_data_parameters %>%
  left_join(just_fluency, by = c("Subject", "Group")) %>%
  left_join(random_nLL_lookup, by = c("Subject","Group")) %>%  
  left_join(parameters, by="parameter_link") %>%
  mutate(
    optimalBIC = k * log(N) - 2 * (-Negative_Log_Likelihood_Optimized),
    randomBIC = random_k * log(N) - 2 * (-random_nLLs))

#lowest BIC model per participant
lowest_bic = bic %>%
  group_by(Subject) %>%
  slice_min(optimalBIC)


# Looking at distribution
# optimal BIC
lowest_bic %>%
  ggplot(aes(x = optimalBIC, fill = Group)) + facet_wrap(~Group)+
  geom_density(fill = "#69b3a2", color = "#e9ecef", alpha = 0.8)+xlim(50,400)

lowest_bic %>%
  ggplot( aes(x=Group, y=optimalBIC, fill=Group)) +
    geom_boxplot() +
    geom_jitter(color="black", size=2, alpha=0.9) +
    theme(
      legend.position="none",
      plot.title = element_text(size=11)) 



#######################################

#BY GROUP BEST MODELS - USE

#median delta BIC
median_delta_bic = bic %>%
 mutate(deltaBIC  = randomBIC - optimalBIC) %>%
  group_by(Group,everything) %>%
  summarise(median_delta_BIC = median(deltaBIC)) %>%
  arrange(desc(median_delta_BIC)) %>%
  slice(1) %>% 
  ungroup() 
kable(median_delta_bic)


# Exploratory - collapsing across groups
#median_delta_bic_all = bic %>%
# mutate(deltaBIC  = randomBIC - optimalBIC) %>%
#  group_by(everything) %>%
# summarise(median_delta_BIC = median(deltaBIC)) %>%
#  arrange(desc(median_delta_BIC)) %>%
#  slice(1) %>% 
#  ungroup() 
#kable(median_delta_bic_all)





```



## Beta Analyses
```{r}

#BY GROUP
#Filtering full data to only include best model for each domain type
best_models <- all_models %>%
  filter(paste(Group,everything) %in% paste(median_delta_bic$Group, median_delta_bic$everything)) %>%
  group_by(Group)

#Separating out betas
beta_values  = best_models  %>%
  pivot_longer(names_to = "beta", cols = c(Beta_Frequency, Beta_Phonological, Beta_Semantic))


# Run lm() for each beta type and extract results
beta_results <- beta_values  %>%
  group_by(beta) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group, data = .)) 
  )


#tbl_df <- beta_results %>%
#  mutate(tidy_model = map(model, ~ broom::tidy(.x, conf.int = TRUE))) %>%
#  unnest(tidy_model) %>%
#transmute(
#  Term = term,
#    Estimate = round(estimate, 3),
#    CI_low = round(conf.low, 3),
#    CI_high = round(conf.high, 3),
#   t_value = round(statistic, 3),
#    SE = round(std.error, 3),
#    p_value = signif(p.value, 3)
#  )
# Write to CSV
#write.csv(tbl_df, "beta_model_results.csv", row.names = FALSE)

#View results
beta_summaries <- beta_results %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(beta_summaries)



##############################
#EXPLORATORY
#Filtering full data to only include best model for all (collapsing across groups)
#best_models_all <- all_models %>%
#  filter(paste(everything) %in% paste(median_delta_bic_all$everything)) 

#Separating out betas
#beta_values_all  = best_models_all  %>%
 # pivot_longer(names_to = "beta", cols = c(Beta_Frequency, Beta_Phonological, Beta_Semantic))


# Run lm() for each beta type and extract results
#beta_results_all <- beta_values_all  %>%
#  group_by(beta) %>%                                   
#  nest() %>%                                         
# mutate(
#    model = map(data, ~ lm(value ~ Group, data = .)) 
#  )


#tbl_df <- beta_results %>%
#  mutate(tidy_model = map(model, ~ broom::tidy(.x, conf.int = TRUE))) %>%
#  unnest(tidy_model) %>%
#transmute(
#  Term = term,
#    Estimate = round(estimate, 3),
#    CI_low = round(conf.low, 3),
#    CI_high = round(conf.high, 3),
#   t_value = round(statistic, 3),
#    SE = round(std.error, 3),
#    p_value = signif(p.value, 3)
#  )
# Write to CSV
#write.csv(tbl_df, "beta_model_results.csv", row.names = FALSE)

#View results
#beta_summaries_all <- beta_results_all %>%
#  mutate(model_summary = map(model, broom::tidy)) %>%
#  unnest(model_summary)
#View(beta_summaries_all)

#semantic and frequency findings the same, group ME for phonological similarity 


```


### Beta plot
```{r}

#BY GROUP BEST MODEL
beta_values %>%
  group_by(Group,beta) %>%
  tidyboot_mean(value, nboot = 1000, na.rm = TRUE) %>%
  separate(beta, into = c("b", "beta")) %>%
  mutate(
    beta = tolower(beta),
    beta = fct_recode(beta, 
                      `Semantic\nSimilarity` = "semantic", 
                      `Phonological\nSimilarity` = "phonological",
                      `Word\nFrequency` = "frequency")
  ) %>%
  mutate(beta = fct_relevel(beta, "Semantic\nSimilarity", "Phonological\nSimilarity", "Word\nFrequency")) %>%
  ggplot(aes(x = beta, y = empirical_stat, group = Group, fill = Group)) +
  geom_bar(stat = 'identity', position = "dodge",color="black") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, position = position_dodge(0.9)) +
  labs(y = bquote(beta ~ "(Parameter Salience)"), x = "") +
  theme_few() +
  theme() +
  scale_fill_paletteer_d("MoMAColors::Koons")

```
############################################################################

# CLUSTERING ANALYSES
```{r}

#BY GROUP
#filtering to only include best method from above
best_clusters <- all_indv%>%
  filter(paste(Group, full_method) %in% paste(best_models$Group, best_models$full_method)) %>%select(-fluency_score)%>% #this is full of NAs & produces mismatch .x/.y error, so dropping
  group_by(Group) %>%left_join(lexical_analysis_data_fluency%>%select(Subject,Group,fluency_score)%>%unique(), by = c("Subject", "Group")) #adding fluency_score back in

#When using the best method for each Group, does avg cluster mean differ by Group x domain
#number of items produced as a covariate (intx does not sig add to model)
cluster_size_lm = lm(data = best_clusters, Cluster_Size_mean ~ Group + fluency_score)
summary(cluster_size_lm)

#tbl_df <- broom::tidy(cluster_size_lm, effects = "fixed", conf.int = TRUE) %>%
#  transmute(
#    Term = term,
#    Estimate = round(estimate, 3),
#    `CI_low` = round(conf.low, 3),
#    `CI_high` = round(conf.high, 3),
#    t_value = round(statistic, 3),
#    SE = round(std.error, 3),
#    p_value = signif(p.value, 3)
#)
#write.csv(tbl_df, "cluster_model_results.csv", row.names = FALSE)

#When using the best method for each Group, does avg numb switches differ by Group x domain
switch_numb_lm = lm(data = best_clusters, Number_of_Switches ~Group + fluency_score)
summary(switch_numb_lm)

#tbl_df <- broom::tidy(switch_numb_lm, effects = "fixed", conf.int = TRUE) %>%
#  transmute(
#   Term = term,
#    Estimate = round(estimate, 3),
#   `CI_low` = round(conf.low, 3),
#    `CI_high` = round(conf.high, 3),
#    t_value = round(statistic, 3),
#   SE = round(std.error, 3),
#    p_value = signif(p.value, 3)
# )
#write.csv(tbl_df, "switchnum_model_results.csv", row.names = FALSE)

#Replicating prev analyses where they don't include fluency score as covariate
cluster_size_lm_no_fs = lm(data = best_clusters, Cluster_Size_mean ~ Group)
summary(cluster_size_lm_no_fs)


#When using the best method for each Group, does avg numb switches differ by Group x domain
switch_numb_lm_no_fs = lm(data = best_clusters, Number_of_Switches ~Group)
summary(switch_numb_lm_no_fs)


###########################
#EXPLORATORY
#looking at clustering using overall best method (collapsing across groups)
#filtering to only include best method for all
#best_clusters_all <- all_indv%>%
#  filter(paste(full_method) %in% paste(best_models_all$full_method)) %>%select(-fluency_score)%>% #this is full of NAs & produces mismatch .x/.y error, so dropping
#  group_by(Group) %>%left_join(lexical_analysis_data_fluency%>%select(Subject,Group,fluency_score)%>%unique(), by = c("Subject", "Group")) #adding fluency_score back in

#When using the best method, does avg cluster mean differ by Group x domain
#number of items produced as a covariate (intx does not sig add to model)
#cluster_size_lm_all = lm(data = best_clusters_all, Cluster_Size_mean ~ Group + fluency_score)
#summary(cluster_size_lm_all)
#no diff from group level

#When using the best method, does avg numb switches differ by Group x domain
#switch_numb_lm_all = lm(data = best_clusters_all, Number_of_Switches ~Group + fluency_score)
#summary(switch_numb_lm_all)
#no diff from group level

#Replicating prev analyses where they don't include fluency score as covariate
#cluster_size_lm_no_fs_all = lm(data = best_clusters_all, Cluster_Size_mean ~ Group)
#summary(cluster_size_lm_no_fs_all)
#no diff from group level

#When using the best method, does avg numb switches differ by Group x domain
#switch_numb_lm_no_fs_all = lm(data = best_clusters_all, Number_of_Switches ~Group)
#summary(switch_numb_lm_no_fs_all)
#no diff from group level


####################

#Breaking out lexical metrics within and across clusters

#filtering to only include best method for each Group
best_switches <- all_switches %>% 
  filter(paste(Group,full_method) %in% paste(best_models$Group, best_models$full_method)) %>%
  mutate(Switch_Value = if_else(Switch_Value == 2, 1, Switch_Value))

#Calculate item_no consistently in all_lexical to use in joining
items_lexical_data <- all_lexical %>%
  group_by(Subject, Group) %>%
  mutate(row_id = row_number() - 1)  # Adjust row_id to start at 0 to account for first item

#this code drops the first item from each list, and if switch_value = 1, marked as a switch trial, if switch_value=0, marked as within cluster transition, also labels clusters and switches by ordinal position
#new first item only marked as within cluster if original switch_value==0
switches_recoded <- best_switches %>%
  group_by(Subject,Group) %>%
  mutate(
    Cluster_Number = cumsum(Switch_Value == 1),
    Switch = case_when(
      row_number() == 1 ~ "Within Cluster Transition",  # Changed to "Within Cluster Transition"
      Cluster_Number != lag(Cluster_Number) ~ "Switch Trial", # Adding cluster number
      TRUE ~ "Within Cluster Transition"
    )
  ) %>%
  mutate(Switch_Number = case_when(
    Switch == "Switch Trial" ~ Cluster_Number - 1,  # Adding switch number
    TRUE ~ NA_real_
  )) %>%
  group_by(Subject,Group) %>%
  mutate(row_id = row_number() - 1) %>%  # Adjusting row_id to start at 0 to be able to drop first item
  left_join(items_lexical_data, by = c("Subject","Group", "Fluency_Item","row_id")) %>%
  rename(
    "semantic" = "Semantic_Similarity",
    "phonological" = "Phonological_Similarity",
    "frequency" = "Frequency_Value"
  ) %>% filter(row_id!=0)%>% #dropping first word of each list
  pivot_longer(names_to = "cue", cols = c(semantic, phonological, frequency))%>%left_join(just_fluency)%>%rename(fluency_score=N)

#write.csv(switches_recoded,"switches.csv")
                                                                                       
                                                                                       
## does semantic similarity, phonological similarity, and frequency differ within clusters vs at switch points for each Group 
# Run lmer() for each cue type and extract results
#fluence score ns so dropped to look at 2 way

switch_results <- switches_recoded  %>%
  group_by(cue) %>%                                   
  nest() %>%                                         
 mutate(
    model = map(data, ~ lm(value ~ Group*Switch*fluency_score, data = .)) 
  )

#View results
#large so calling indp
print("Semantic")
switch_results$model[[1]] %>% summary()
print("phonological")
switch_results$model[[2]] %>% summary()
print("Frequency")
switch_results$model[[3]] %>% summary()

```

## Cluster Plots
```{r}
#Avg cluster size
clusters<-emmip(cluster_size_lm,Group ~ fluency_score, cov.reduce = range, CI = TRUE) + 
  guides(color = guide_legend(title = NULL)) +
  theme_few() +
  scale_colour_paletteer_d("lisa::SandroBotticelli") +
  labs(
    x = "Fluency score",
    y = "Predicted cluster size") +theme(aspect.ratio=.75,legend.position="right")

# Number of switches
switches<-emmip(switch_numb_lm, Group ~ fluency_score, cov.reduce = range,CI=TRUE)+ 
      guides(color = guide_legend(title = NULL)) +  
      theme_few() + scale_colour_paletteer_d("lisa::SandroBotticelli") +
  labs(
    x = "Fluency score",
    y = "Predicted number of switches")+theme(aspect.ratio=.75)

clusters+switches+ plot_layout(ncol = 1)

#Within cluster vs switch lexical plots
plots_switch<-switch_results %>% 
  mutate(
    emmip_plot = map(model, ~ 
      emmip(.x, Group ~ Switch ,  
            cov.reduce = range, CI = TRUE) +
      labs(title = "", x = "Switch", y = paste("Estimated", unique(cue)))
      +scale_colour_paletteer_d("tvthemes::Alexandrite")  +
      scale_fill_paletteer_d("tvthemes::Alexandrite") +
      theme_few() + 
      theme(legend.position = "top")))

plots_switch$emmip_plot[[1]]
plots_switch$emmip_plot[[2]]
plots_switch$emmip_plot[[3]]

#phon sim and fluency score
ggplot(data=switches_recoded%>%filter(cue=="phonological"),aes(y = fluency_score, x = value, group = Switch, color = Switch)) + facet_wrap(~Group)+
  theme_few() +
  geom_smooth(method = "lm", se = TRUE,size=1) +
  theme(legend.position="top",
    aspect.ratio = 1,  
    axis.text.x = element_text( hjust = 1, face = "bold"))+labs(x = "Phonological Similarity", y = "Fluency Score")


#looking at frequency
ggplot(data=switches_recoded%>%filter(cue=="frequency"),aes(x = Group, y = value, group = Switch, color = Switch)) + 
  theme_few() +
  geom_smooth(method = "lm", se = TRUE,size=1) +
  theme(legend.position="top",
    aspect.ratio = 1,  
    axis.text.x = element_text( hjust = 1, face = "bold"))+labs(y = "frequency", x = "Group")


```
################################################################
# PWA BATTERY MEASURES
```{r}

#Battery descriptions & means from Mirman et al. (2015)

#Philadelphia Naming Test (PNT)
#A 175-item single-word picture naming test using drawings, target words cover a relatively wide range of word length, word
#frequency, and semantic category. The pictures are all familiar objects with high name agreement (97%)
#Overall percent correct (M=63.3, SD=29.1, Range=1.1–97.7)
#Percent of semantic errors (M=5.4, SD=3.9, Range=1.1–18.3)
#percent of phonological errors (M=13.2, SD=13.3, Range=0–49.1).

#Philadelphia Repetition Test
#A word repetition test using the same set of 175 targets as the Philadelphia Naming Test. 
#Performance is measured by percent correct: M=85.9, SD=14.1, Range=39–100.

#Camels and Cactus Test (camcac.)
#Test of non-verbal semantic processing in which a pictured item must be matched to the closest associate 
#among a set of four pictured choices (e.g., wine matched to: grapes, cherry, strawberry,orange). 
#Performance is measured by percent correct of 64 trials: M=75.2, SD=15.0, Range=25–95.

#Synonymy triplets (syntot.)
#Test of verbal semantic processing in which participants must decide which two of three words are most similar in meaning. Half
#the trials involve nouns (e.g., violin, fiddle, clarinet), the other half
#verbs (e.g., to repair, to design, to fix). Performance is measured by percent correct of 30 trials: M=79.1, SD=16.9, Range=33–100.


#Nonword Repetition Test (nonwdrep..)
#Pre-recorded nonword targets derived from PNT,  target words were presented to participants for repetition. 
#Performance is measured by percent correct of 60 trials: M=47.3, SD=25.8, Range=0–98.


#na counts
na_table <- tibble(
  column = names(pwa_dx),
  n_NA = sapply(pwa_dx, function(x) sum(is.na(x)))
) %>%
  filter(n_NA > 0) %>%
  arrange(desc(n_NA))

na_table

#set up dfs
battery_long <- pwa_dx %>%
  group_by(Subject) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  filter(Subject %in% all_indv$Subject) %>%
  select(Subject, PNT_Total_Correct, Semantic_Errors_PNT, Formal_Errors_PNT, Mixed_Errors_PNT, Nonword_Errors_PNT,Nonword_Repetition_Test, PRT, Synonymy_Triplet_Test, Camels_and_Cactus_Test,Immediate_Serial_Recall_Wordspan,Category_Probe,Maximum_Category_Probe,Rhyme_Probe,Maximum_Rhyme_Probe) %>%
  mutate(
    Subject = as.factor(Subject),
    across(-Subject, ~ suppressWarnings(as.numeric(as.character(.))))
  ) %>%
  mutate(
    semantic_output = rowSums(across(c(Semantic_Errors_PNT, Mixed_Errors_PNT)), na.rm = TRUE),
    phon_output     = rowSums(across(c(Nonword_Errors_PNT, Formal_Errors_PNT)), na.rm = TRUE)
  ) %>%
  select(-Semantic_Errors_PNT, -Mixed_Errors_PNT, -Formal_Errors_PNT) %>%
  pivot_longer( # put in long format for outlier drops
    cols = c(PNT_Total_Correct, semantic_output,phon_output, Nonword_Repetition_Test, PRT, Synonymy_Triplet_Test, Camels_and_Cactus_Test,Immediate_Serial_Recall_Wordspan,Category_Probe,Maximum_Category_Probe,Rhyme_Probe,Maximum_Rhyme_Probe),
    names_to = "test",
    values_to = "value"
  ) %>%
  group_by(test) %>% #remove outliers via MAD
  filter(!is.na(value)) %>%
  mutate(
    MAD = 3 * mad(value, na.rm = TRUE),
    med = median(value, na.rm = TRUE),
    L   = med - MAD,
    U   = med + MAD,
    is_outlier = (value < L | value > U))

# Keep the filtered data (no outliers), put into wide format
battery_clean<- battery_long %>%
  filter(!is_outlier) %>%
  select(-med, -MAD, -L, -U, -is_outlier) %>%
  pivot_wider(
    id_cols = Subject,
    names_from = test,
    values_from = value)


# check dropped values by test
battery_long %>%
  filter(is_outlier) %>%
  count(test, name = "n_outliers") %>%
  arrange(desc(n_outliers))

#LOOKING AT CORRELATIONS
library(corrplot)
corrplot(cor(battery_clean%>%select(-Subject)%>%na.omit()))

#####

#set up dfs

#getting word lengths
pwa_avg_length<-pwa_lengths %>%  group_by(Subject) %>%
  summarize(avg_length = mean(Phoneme.Count, na.rm = TRUE))

# Lexical wide
lexical_battery_wide<-all_indv %>% select(Subject, Group, Semantic_Similarity_mean, Phonological_Similarity_mean, Frequency_Value_mean, 
         fluency_score) %>% na.omit() %>%filter(Group=="PWA") %>%left_join(battery_clean,by="Subject") %>% left_join(pwa_avg_length,by="Subject")%>%
  rename("semantic" = "Semantic_Similarity_mean", 
         "phonological" = "Phonological_Similarity_mean",
         "frequency" = "Frequency_Value_mean")

#long
lexical_battery_long<-lexical_battery_wide %>%
 pivot_longer(names_to = "cue", cols=c(semantic,phonological,frequency))
  


#Clustering df
clusters_battery<-best_clusters %>%filter(Group=="PWA") %>%left_join(battery_clean,by="Subject")
#write.csv(clusters_battery,"clustering_metrics_battery.csv") # save to csv




```
## Lexical Analyses
```{r}
#do battery measures predict fluency
fluency_battery_lm = lm(data = lexical_battery_wide, fluency_score ~ PNT_Total_Correct+semantic_output+phon_output+PRT+Synonymy_Triplet_Test+Camels_and_Cactus_Test+Nonword_Repetition_Test+avg_length+Immediate_Serial_Recall_Wordspan+Category_Probe+Maximum_Category_Probe+Rhyme_Probe+Maximum_Rhyme_Probe)
summary(fluency_battery_lm)

#PNT performance, nonword repetition, wordspan, and rhyme probe all predict fluency (wordspan effect negative)
#if STM/WM is more intact, related competitors may be more coactive & cause interference during SFT?

#tbl_df <- broom::tidy(fluency_battery_lm, effects = "fixed", conf.int = TRUE) %>%
#transmute(
#    Term = term,
#    Estimate = round(estimate, 3),
#  `CI_low` = round(conf.low, 3),
#    `CI_high` = round(conf.high, 3),
#    t_value = round(statistic, 3),
#    SE = round(std.error, 3),
#    p_value = signif(p.value, 3)
# )
#write.csv(tbl_df, "fluency_battery_model_results.csv", row.names = FALSE)

#Do battery measures predict semantic similarity, phonological similarity, and word frequency
# Run lm() for each cue and extract results
lexical_battery_results <- lexical_battery_long  %>% 
  group_by(cue) %>%                                   # Group by cue
  nest() %>%                                         # Nest data by cue
 mutate(
    model = map(data, ~ lm(value ~ PNT_Total_Correct+semantic_output+phon_output+PRT+Synonymy_Triplet_Test+Camels_and_Cactus_Test+Nonword_Repetition_Test+avg_length+Immediate_Serial_Recall_Wordspan+Category_Probe+Maximum_Category_Probe+Rhyme_Probe+Maximum_Rhyme_Probe+fluency_score, data = .)) # Fit models
  )
#View results
model_battery_summaries <- lexical_battery_results %>%
  mutate(model_summary = map(model, broom::tidy)) %>%
  unnest(model_summary)
View(model_battery_summaries)

#catspan - the smaller the semantic working memory span, the higher the average frequency in SFT
#higher word span asc w/ greater avg word frequency

#tbl_df <- lexical_battery_results %>%
#  mutate(tidy_model = map(model, ~ broom::tidy(.x, conf.int = TRUE))) %>%
#  unnest(tidy_model) %>%
#  transmute(
#    Cue = cue,
#   Term = term,
#    Estimate = round(estimate, 3),
#    CI_low = round(conf.low, 3),
#    CI_high = round(conf.high, 3),
#    t_value = round(statistic, 3),
#   SE = round(std.error, 3),
#    p_value = signif(p.value, 3)
#  )

# Write to CSV
#write.csv(tbl_df, "lexical_battery_model_results.csv", row.names = FALSE)

#for verbal semantic processing/comprehension tasks (syntot), high performance assc with lower inter-item phonological similarity in SFT  (less interference/more control over semantic processing)
#for NON verbal semantic association/processing tasks (camcac), high score assc with high avg inter-item phonological similarity in SFT

```

## FLUENCY LASSO

```{r}
# ============================================================
# ============================================================
# 1) RELAXED LASSO (complete cases)

run_relaxed_lasso_fluency <- function(df, outcome = "fluency_score", preds) {

  # Keep only outcome + predictors, drop rows with any NA (glmnet can't handle NA)
  d <- df %>%
    select(all_of(c(outcome, preds))) %>%
    na.omit()

  if (nrow(d) < 15) return(NULL)

  # Build glmnet x/y
  x <- model.matrix(reformulate(preds, response = outcome), data = d)[, -1]
  y <- d[[outcome]]

  # CV relaxed LASSO
  set.seed(123)
  cv <- cv.glmnet(
    x, y,
    alpha = 1,
    relax = TRUE,
    standardize = TRUE,
    nfolds = 10
  )

  # Helper to extract non-zero coefficient names
  get_selected <- function(s) {
    b <- coef(cv, s = s, mode = "relax")
    setdiff(rownames(b)[as.numeric(b) != 0], "(Intercept)")
  }

  list(
    n_complete   = nrow(d),
    lambda_1se   = cv$lambda.1se,
    lambda_min   = cv$lambda.min,
    selected_1se = get_selected("lambda.1se"),
    selected_min = get_selected("lambda.min")
  )
}
preds <- c(
  "PNT_Total_Correct","semantic_output","phon_output","PRT",
  "Synonymy_Triplet_Test","Camels_and_Cactus_Test","Nonword_Repetition_Test",
  "avg_length","Immediate_Serial_Recall_Wordspan",
  "Category_Probe","Maximum_Category_Probe",
  "Rhyme_Probe","Maximum_Rhyme_Probe"
)
fluency_relaxed <- run_relaxed_lasso_fluency(
  df = lexical_battery_wide,
  outcome = "fluency_score",
  preds = preds
)

fluency_relaxed

cat("\nComplete rows used:", fluency_relaxed$n_complete, "\n")
cat("\nlambda.1se:", fluency_relaxed$lambda_1se, "\n")
cat("lambda.min:", fluency_relaxed$lambda_min, "\n")

cat("\n--- Selected @ lambda.1se ---\n")
print(fluency_relaxed$selected_1se)

cat("\n--- Selected @ lambda.min ---\n")
print(fluency_relaxed$selected_min)


refit_ols <- function(df, outcome, selected_vars) {
  if (length(selected_vars) == 0) return(NULL)
  d <- df %>% select(all_of(c(outcome, selected_vars))) %>% na.omit()
  fit <- lm(reformulate(selected_vars, response = outcome), data = d)
  summary(fit)
}

summary_1se <- refit_ols(lexical_battery_wide, "fluency_score", fluency_relaxed$selected_1se)
summary_min <- refit_ols(lexical_battery_wide, "fluency_score", fluency_relaxed$selected_min)

summary_1se
summary_min

#MI LASSO

# -------------------------
# 1) Choose predictors
# -------------------------
vars <- c(
  "fluency_score",
  "PNT_Total_Correct","semantic_output","phon_output","PRT",
  "Synonymy_Triplet_Test","Camels_and_Cactus_Test","Nonword_Repetition_Test",
  "avg_length","Immediate_Serial_Recall_Wordspan",
  "Category_Probe","Maximum_Category_Probe",
  "Rhyme_Probe","Maximum_Rhyme_Probe"
)

# -------------------------
# 2) Prep data for glmnet
#    - glmnet cannot handle NAs, so drop incomplete rows
#    - glmnet needs:
#        x = numeric matrix of predictors
#        y = numeric outcome vector
# -------------------------
df <- lexical_battery_wide[, vars] %>% na.omit()
cat("Rows after na.omit():", nrow(df), "\n")

# model.matrix() creates the predictor matrix the same way lm() would
# remove the intercept column because glmnet adds it internally
x <- model.matrix(fluency_score ~ ., data = df)[, -1]
y <- df$fluency_score


##Multiple Imputation LASSO to handle missing data
library(mice)
imp <- mice(lexical_battery_wide[, vars], m = 20, seed = 123)

lasso_selected <- lapply(1:20, function(i) {
  d <- complete(imp, i)
  x <- model.matrix(fluency_score ~ ., data = d)[, -1]
  y <- d$fluency_score
  cv <- cv.glmnet(x, y, alpha = 1)
  rownames(coef(cv, s = "lambda.min"))[
    coef(cv, s = "lambda.min")[,1] != 0
  ]
})

table(unlist(lasso_selected))

####

## PCA
library(psych)

X <- df[, setdiff(names(df), "fluency_score")]

# check suitability
KMO(X)
cortest.bartlett(cor(X), n = nrow(X))

# exploratory factor analysis
fa_fit <- fa(X, nfactors = 3, rotate = "oblimin", fm = "ml")
print(fa_fit$loadings, cutoff = .3)

# factor scores
scores <- factor.scores(X, fa_fit)$scores
scores_df <- as.data.frame(scores)
scores_df$fluency_score <- df$fluency_score

pca_fluency<-lm(fluency_score ~ ., data = scores_df)
summary(pca_fluency)


# ============================================================
# Train/Test MI-LASSO
# 1) Multiple Imputation (MICE)
# 2) Train/Validation split
# 3) LASSO (10-fold CV) on training set for variable selection
# 4) Refit classical regression on selected vars
# 5) Keep vars with p < .05
# 6) VIF screening (VIF < 3)
# 7) Final regression on training set
# 8) Evaluate performance on validation set

# ============================================================

suppressPackageStartupMessages({
  library(dplyr)
  library(glmnet)
  library(mice)
  library(caret)
  library(car)
})

# -------------------------
# 0) Specify variables
# -------------------------
vars <- c(
  "fluency_score",
  "PNT_Total_Correct","semantic_output","phon_output","PRT",
  "Synonymy_Triplet_Test","Camels_and_Cactus_Test",
  "Nonword_Repetition_Test","avg_length"
)

# -------------------------
# 1) Multiple imputation (MICE)
# -------------------------
imp <- mice(
  lexical_battery_wide[, vars],
  m = 20,
  seed = 123,
  print = FALSE
)

# Many papers proceed by analyzing one completed dataset.
# If you want to loop over all imputations, see optional section at bottom.
data_imp <- complete(imp, 1)

# -------------------------
# 2) Train / validation split
# -------------------------
set.seed(123)
train_idx <- createDataPartition(data_imp$fluency_score, p = 0.70, list = FALSE)
train <- data_imp[train_idx, ]
valid <- data_imp[-train_idx, ]

# -------------------------
# 3) LASSO on training set (10-fold CV)
# -------------------------
x_train <- model.matrix(fluency_score ~ ., data = train)[, -1]  # drop intercept
y_train <- train$fluency_score

set.seed(123)
cv_lasso <- cv.glmnet(
  x_train, y_train,
  alpha = 1,      # LASSO
  nfolds = 10,
  standardize = TRUE
)

# Choose lambda: "lambda.min" matches your MI-LASSO earlier; lambda.1se is more conservative
lambda_choice <- "lambda.min"

# Extract selected predictors (non-zero coefficients)
b <- coef(cv_lasso, s = lambda_choice)
selected_vars <- rownames(b)[as.numeric(b) != 0]
selected_vars <- setdiff(selected_vars, "(Intercept)")

cat("\n==============================\n")
cat("LASSO-selected predictors @", lambda_choice, "\n")
cat("==============================\n")
print(selected_vars)

if (length(selected_vars) == 0) {
  stop("LASSO selected no predictors at the chosen lambda. Try lambda.1se or check data.")
}

# -------------------------
# 4) Refit classical OLS on selected predictors (training set)
# -------------------------
f_selected <- as.formula(paste("fluency_score ~", paste(selected_vars, collapse = " + ")))
lm_selected <- lm(f_selected, data = train)

cat("\n==============================\n")
cat("OLS refit on LASSO-selected predictors (TRAIN)\n")
cat("==============================\n")
print(summary(lm_selected))

# -------------------------
# 5) Keep predictors with p < .05 (as in the example paper)
# -------------------------
pvals <- summary(lm_selected)$coefficients[, "Pr(>|t|)"]
sig_vars <- names(pvals)[pvals < 0.05]
sig_vars <- setdiff(sig_vars, "(Intercept)")

cat("\n==============================\n")
cat("Predictors with p < .05 after OLS refit\n")
cat("==============================\n")
print(sig_vars)

if (length(sig_vars) == 0) {
  cat("\nNo predictors meet p < .05 after refit. Stopping before VIF step.\n")
  # You can still evaluate the lm_selected model if you want:
  # goto evaluation section below
}

# -------------------------
# 6) VIF screening (VIF < 3)
#    Only possible if there are at least 2 predictors
# -------------------------
lm_sig <- NULL
final_vars <- character(0)

if (length(sig_vars) >= 2) {
  f_sig <- as.formula(paste("fluency_score ~", paste(sig_vars, collapse = " + ")))
  lm_sig <- lm(f_sig, data = train)

  vif_vals <- car::vif(lm_sig)

  cat("\n==============================\n")
  cat("VIF values (TRAIN)\n")
  cat("==============================\n")
  print(vif_vals)

  final_vars <- names(vif_vals)[vif_vals < 3]

  cat("\n==============================\n")
  cat("Final predictors after VIF < 3\n")
  cat("==============================\n")
  print(final_vars)

} else if (length(sig_vars) == 1) {
  cat("\nOnly 1 predictor passed p < .05, so VIF is not defined.\n")
  cat("Keeping the single predictor:\n")
  print(sig_vars)
  final_vars <- sig_vars

} else {
  cat("\nNo predictors passed p < .05, skipping VIF.\n")
  final_vars <- character(0)
}

# -------------------------
# 7) Fit FINAL model (training set)
# -------------------------
lm_final <- NULL

if (length(final_vars) > 0) {
  f_final <- as.formula(paste("fluency_score ~", paste(final_vars, collapse = " + ")))
  lm_final <- lm(f_final, data = train)

  cat("\n==============================\n")
  cat("FINAL OLS model (TRAIN)\n")
  cat("==============================\n")
  print(summary(lm_final))
} else {
  cat("\nNo final predictors after p-value and/or VIF filtering.\n")
  cat("Consider using the LASSO-selected OLS refit (lm_selected) as your final model.\n")
}

# -------------------------
# 8) Evaluate on VALIDATION set
#    We'll evaluate whichever model exists:
#      - lm_final if it exists
#      - else lm_selected
# -------------------------
eval_model <- function(fit, newdata, outcome = "fluency_score") {
  pred <- predict(fit, newdata = newdata)
  y <- newdata[[outcome]]

  rmse <- sqrt(mean((y - pred)^2))
  r2 <- 1 - sum((y - pred)^2) / sum((y - mean(y))^2)

  c(RMSE = rmse, R2 = r2)
}

cat("\n==============================\n")
cat("VALIDATION PERFORMANCE\n")
cat("==============================\n")

if (!is.null(lm_final)) {
  cat("\nUsing FINAL model:\n")
  print(eval_model(lm_final, valid))
} else {
  cat("\nUsing LASSO-selected OLS refit model (since FINAL model was empty):\n")
  print(eval_model(lm_selected, valid))
}

# ============================================================
# OPTIONAL: If you want to repeat the entire pipeline across all
# imputations and summarize stability of selection / performance,
# tell me and I’ll give the pooled version (it’s longer).
# ============================================================



lm_fit <- lm(
  fluency_score ~ PNT_Total_Correct + semantic_output + phon_output + PRT +
    Synonymy_Triplet_Test + Camels_and_Cactus_Test + Nonword_Repetition_Test +
    avg_length + Immediate_Serial_Recall_Wordspan +
    Category_Probe + Maximum_Category_Probe +
    Rhyme_Probe + Maximum_Rhyme_Probe,
  data = lexical_battery_wide
)

vif_vals <- vif(lm_fit)
vif_vals

vif_table <- data.frame(
  Predictor = names(vif_vals),
  VIF = as.numeric(vif_vals),
  row.names = NULL
)

print(vif_table)
```

# Lexical LASSO
```{r}
# LASSO
preds <- c(
  "PNT_Total_Correct","semantic_output","phon_output","PRT",
  "Synonymy_Triplet_Test","Camels_and_Cactus_Test",
  "Nonword_Repetition_Test","avg_length",
  "Immediate_Serial_Recall_Wordspan",
  "Category_Probe","Maximum_Category_Probe",
  "Rhyme_Probe","Maximum_Rhyme_Probe"
)

# ============================================================
# MULTIPLE-IMPUTATION LASSO
# ============================================================
run_mi_lasso <- function(df, m = 20) {

  d <- df %>% select(value, all_of(preds))
  if (nrow(d) < 15) return(NULL)

  imp <- mice(d, m = m, seed = 123, print = FALSE)

  selected <- map(1:m, function(i) {
    di <- complete(imp, i)
    x <- model.matrix(value ~ ., data = di)[, -1]
    y <- di$value
    cv <- cv.glmnet(x, y, alpha = 1)
    setdiff(
      rownames(coef(cv, s = "lambda.min"))[
        coef(cv, s = "lambda.min")[,1] != 0
      ],
      "(Intercept)"
    )
  })

  # frequency table of how often each predictor is selected
  sort(table(unlist(selected)), decreasing = TRUE)
}

# ============================================================
# 3) PCA / FACTOR REGRESSION
#    - ML factor analysis with oblimin rotation
#    - regress outcome on factor scores
# ============================================================
run_pca_regression <- function(df) {

  d <- df %>%
    select(value, all_of(preds)) %>%
    na.omit()

  # Guard against tiny samples
  if (nrow(d) < 20) return(NULL)

  X <- d[, preds]

  # ------------------------------------------------------------
  # 1) Parallel analysis to determine number of factors
  #    plot = FALSE avoids graphics issues in pipelines
  # ------------------------------------------------------------
  pa <- fa.parallel(
    X,
    fm = "ml",
    fa = "fa",
    plot = FALSE
  )

  n_factors <- pa$nfact

  # If parallel analysis fails or returns 0, stop safely
  if (is.null(n_factors) || n_factors < 1) return(NULL)

  # ------------------------------------------------------------
  # 2) Fit factor analysis using data-supported number of factors
  # ------------------------------------------------------------
  fa_fit <- fa(
    X,
    nfactors = n_factors,
    rotate = "oblimin",
    fm = "ml"
  )

  # ------------------------------------------------------------
  # 3) Compute factor scores and regress outcome on them
  # ------------------------------------------------------------
  scores <- factor.scores(X, fa_fit)$scores
  scores_df <- as.data.frame(scores)
  scores_df$value <- d$value

  lm_fit <- lm(value ~ ., data = scores_df)

  # ------------------------------------------------------------
  # 4) Return everything needed for inspection / reporting
  # ------------------------------------------------------------
  list(
    n_factors  = n_factors,
    loadings   = fa_fit$loadings,
    lm_summary = summary(lm_fit)
  )
}

# ============================================================
# 4) RUN ALL ANALYSES PER CUE
# ============================================================
lexical_battery_results <- lexical_battery_long %>%
  group_by(cue) %>%
  nest() %>%
  mutate(
    relaxed_lasso = map(data, run_relaxed_lasso),
    mi_lasso      = map(data, run_mi_lasso),
    pca_model     = map(data, run_pca_regression)
  )

# ============================================================
# 5) EXAMPLES: HOW TO INSPECT RESULTS
# ============================================================

# MI-LASSO selection frequencies
lexical_battery_results %>%
  select(cue, mi_lasso) %>%
  pwalk(function(cue, mi_lasso) {
    cat("\n====================\n")
    cat("Cue:", cue, "\n")
    cat("====================\n")

    if (is.null(mi_lasso)) {
      cat("No MI-LASSO run (too few observations)\n")
    } else {
      print(mi_lasso)
    }
  })


# PCA regression summary for first cue
lexical_battery_results$pca_model[[2]]$lm_summary

# PCA loadings for first cue
lexical_battery_results$pca_model[[2]]$loadings





# ============================================================
# Cue-wise models (lexical_battery_long) using the SAME STYLE
# as the fluency relaxed-LASSO workflow:
#   - Outcome: value  (within each cue)
#   - Predictors: fluency_score + battery measures
#   - Runs PER CUE:
#       1) Complete-case RELAXED LASSO (CV glmnet, relax=TRUE)
#       2) Optional: MI-LASSO selection frequencies (MICE + cv.glmnet)
#       3) Optional: OLS refit on selected predictors (interpretability)
#   - Includes helpers to PRINT and to UNNEST results cleanly
# ============================================================

suppressPackageStartupMessages({
  library(dplyr)
  library(purrr)
  library(tidyr)
  library(glmnet)
  library(mice)
  library(car)    # only if you want VIFs in OLS refits
})

# -------------------------
# 0) Predictor set
# -------------------------
battery_preds <- c(
  "PNT_Total_Correct","semantic_output","phon_output","PRT",
  "Synonymy_Triplet_Test","Camels_and_Cactus_Test",
  "Nonword_Repetition_Test","avg_length",
  "Category_Probe","Maximum_Category_Probe",
  "Rhyme_Probe","Maximum_Rhyme_Probe"
)

# Include fluency_score as a predictor of lexical-cue outcome "value"
preds <- c("fluency_score", battery_preds)

# ============================================================
# 1) Complete-case RELAXED LASSO per cue
# ============================================================
run_relaxed_lasso_value <- function(df, outcome = "value", preds, min_n = 20) {

  d <- df %>%
    select(all_of(c(outcome, preds))) %>%
    na.omit()

  if (nrow(d) < min_n) return(NULL)

  x <- model.matrix(reformulate(preds, response = outcome), data = d)[, -1]
  y <- d[[outcome]]

  set.seed(123)
  cv <- cv.glmnet(
    x, y,
    alpha = 1,          # LASSO
    relax = TRUE,       # relaxed refit
    standardize = TRUE,
    nfolds = 10
  )

  get_selected <- function(s) {
    b <- coef(cv, s = s, mode = "relax")
    setdiff(rownames(b)[as.numeric(b) != 0], "(Intercept)")
  }

  list(
    n_complete   = nrow(d),
    lambda_1se   = cv$lambda.1se,
    lambda_min   = cv$lambda.min,
    selected_1se = get_selected("lambda.1se"),
    selected_min = get_selected("lambda.min")
  )
}

# ============================================================
# 2) Optional: MI-LASSO per cue (selection stability)
#    - This is NOT relaxed LASSO; it's the common MI-LASSO approach:
#        mice() -> cv.glmnet(alpha=1) each imputation -> selection freq
# ============================================================
run_mi_lasso_value <- function(df, outcome = "value", preds, m = 20, min_n = 20) {

  d <- df %>% select(all_of(c(outcome, preds)))
  if (nrow(d) < min_n) return(NULL)

  # MICE can fail if a column is all-NA or constant in a cue; catch safely
  imp <- try(
    mice(d, m = m, seed = 123, print = FALSE),
    silent = TRUE
  )
  if (inherits(imp, "try-error")) return(NULL)

  selected <- map(1:m, function(i) {
    di <- complete(imp, i)
    x <- model.matrix(reformulate(preds, response = outcome), data = di)[, -1]
    y <- di[[outcome]]
    cv <- cv.glmnet(x, y, alpha = 1, standardize = TRUE, nfolds = 10)

    b <- coef(cv, s = "lambda.min")
    setdiff(rownames(b)[as.numeric(b) != 0], "(Intercept)")
  })

  # selection frequency table
  sort(table(unlist(selected)), decreasing = TRUE)
}

# ============================================================
# 3) Optional: OLS refit on selected predictors (interpretability)
#    - Use ONLY as descriptive follow-up (post-selection p-values are not valid inference)
# ============================================================
refit_ols_value <- function(df, outcome = "value", selected_vars, min_n = 20) {
  if (length(selected_vars) == 0) return(NULL)

  d <- df %>%
    select(all_of(c(outcome, selected_vars))) %>%
    na.omit()

  if (nrow(d) < min_n) return(NULL)

  fit <- lm(reformulate(selected_vars, response = outcome), data = d)

  list(
    n_complete = nrow(d),
    summary    = summary(fit),
    vif        = if (length(selected_vars) >= 2) car::vif(fit) else NA
  )
}

# ============================================================
# 4) Run everything per cue
# ============================================================
lexical_cue_models <- lexical_battery_long %>%
  group_by(cue) %>%
  nest() %>%
  mutate(
    relaxed_lasso = map(data, ~ run_relaxed_lasso_value(.x, outcome = "value", preds = preds, min_n = 20)),
    mi_lasso      = map(data, ~ run_mi_lasso_value(.x, outcome = "value", preds = preds, m = 20, min_n = 20)),
    # OLS refit uses relaxed-lasso selections (lambda.min by default here)
    ols_refit_min = map2(data, relaxed_lasso, ~ {
      if (is.null(.y)) return(NULL)
      refit_ols_value(.x, outcome = "value", selected_vars = .y$selected_min, min_n = 20)
    })
  )

# ============================================================
# 5) How to LOOK INSIDE the list columns (printing helpers)
# ============================================================

# ---- A) Print relaxed LASSO selections cue-by-cue ----
lexical_cue_models %>%
  select(cue, relaxed_lasso) %>%
  pwalk(function(cue, relaxed_lasso) {
    cat("\n====================\n")
    cat("Cue:", cue, "\n")
    cat("====================\n")

    if (is.null(relaxed_lasso)) {
      cat("Relaxed LASSO: NULL (too few complete rows)\n")
      return()
    }

    cat("Complete rows used:", relaxed_lasso$n_complete, "\n")
    cat("lambda.1se:", relaxed_lasso$lambda_1se, "\n")
    cat("lambda.min:", relaxed_lasso$lambda_min, "\n")

    cat("\nSelected @ lambda.1se:\n")
    print(relaxed_lasso$selected_1se)

    cat("\nSelected @ lambda.min:\n")
    print(relaxed_lasso$selected_min)
  })

# ---- B) Print MI-LASSO frequency tables cue-by-cue ----
lexical_cue_models %>%
  select(cue, mi_lasso) %>%
  pwalk(function(cue, mi_lasso) {
    cat("\n====================\n")
    cat("Cue:", cue, "\n")
    cat("====================\n")

    if (is.null(mi_lasso)) {
      cat("MI-LASSO: NULL (imputation failed or too few rows)\n")
    } else {
      print(mi_lasso)
    }
  })

# ---- C) Print OLS refit summaries (lambda.min-selected predictors) ----
lexical_cue_models %>%
  select(cue, ols_refit_min) %>%
  pwalk(function(cue, ols_refit_min) {
    cat("\n====================\n")
    cat("Cue:", cue, "\n")
    cat("====================\n")

    if (is.null(ols_refit_min)) {
      cat("OLS refit: NULL (no predictors selected or too few rows)\n")
      return()
    }

    cat("Complete rows used:", ols_refit_min$n_complete, "\n\n")
    print(ols_refit_min$summary)

    cat("\nVIF (if >=2 predictors):\n")
    print(ols_refit_min$vif)
  })

# ============================================================
# 6) Make tidy tables (so you can View()/export)
# ============================================================

# ---- Relaxed LASSO selections as tidy tables ----
relaxed_tidy <- lexical_cue_models %>%
  transmute(
    cue = cue,
    n_complete = map_int(relaxed_lasso, function(z) {
      if (is.null(z)) NA_integer_ else z$n_complete
    }),
    selected_1se = map(relaxed_lasso, function(z) {
      if (is.null(z)) character(0) else z$selected_1se
    }),
    selected_min = map(relaxed_lasso, function(z) {
      if (is.null(z)) character(0) else z$selected_min
    })
  )

# Unnest into long format (tidyr >= 1.0 style)
relaxed_1se_long <- relaxed_tidy %>%
  select(cue, selected_1se) %>%
  unnest(cols = selected_1se)

relaxed_min_long <- relaxed_tidy %>%
  select(cue, selected_min) %>%
  unnest(cols = selected_min)


# Unnest if you want a long table
relaxed_1se_long <- relaxed_tidy %>% select(cue, selected_1se) %>% unnest(cols = c(selected_1se))
relaxed_min_long <- relaxed_tidy %>% select(cue, selected_min) %>% unnest(cols = c(selected_min))

# ---- MI-LASSO as tidy long table (cue, predictor, times_selected) ----
mi_tidy <- lexical_cue_models %>%
  select(cue, mi_lasso) %>%
  filter(!map_lgl(mi_lasso, is.null)) %>%
  mutate(mi_lasso = map(mi_lasso, as.data.frame)) %>%
  unnest(cols = c(mi_lasso)) %>%
  rename(predictor = Var1, times_selected = Freq) %>%
  mutate(prop_selected = times_selected / 20)

# Examples:
# View(relaxed_tidy)
# View(mi_tidy)
# mi_tidy %>% arrange(cue, desc(times_selected))

```



## Battery x Fluency Plots 
```{r}
emm_results <- list(
  PNT_Total_Correct = emmeans(
    fluency_battery_lm, ~ PNT_Total_Correct, cov.reduce = range
  ),
  semantic_output = emmeans(
    fluency_battery_lm, ~ semantic_output, cov.reduce = range
  ),
  phon_output = emmeans(
    fluency_battery_lm, ~ phon_output, cov.reduce = range
  ),
  PRT = emmeans(
    fluency_battery_lm, ~ PRT, cov.reduce = range
  ),
  Synonymy_Triplet_Test = emmeans(
    fluency_battery_lm, ~ Synonymy_Triplet_Test, cov.reduce = range
  ),
  Camels_and_Cactus_Test = emmeans(
    fluency_battery_lm, ~ Camels_and_Cactus_Test, cov.reduce = range
  ),
  Nonword_Repetition_Test = emmeans(
    fluency_battery_lm, ~ Nonword_Repetition_Test, cov.reduce = range
  ),
  avg_length = emmeans(
    fluency_battery_lm, ~ avg_length, cov.reduce = range
  ),
  Immediate_Serial_Recall_Wordspan = emmeans(
    fluency_battery_lm, ~ Immediate_Serial_Recall_Wordspan, cov.reduce = range
  ),
  Category_Probe = emmeans(
    fluency_battery_lm, ~ Category_Probe, cov.reduce = range
  ),
  Maximum_Category_Probe = emmeans(
    fluency_battery_lm, ~ Maximum_Category_Probe, cov.reduce = range
  ),
  Rhyme_Probe = emmeans(
    fluency_battery_lm, ~ Rhyme_Probe, cov.reduce = range
  ),
  Maximum_Rhyme_Probe = emmeans(
    fluency_battery_lm, ~ Maximum_Rhyme_Probe, cov.reduce = range
  )
)

# -------------------------
# Plot significant predictors
# -------------------------

# Phonological output errors
plot_phon <- emmip(
  emm_results$Camels_and_Cactus_Test,
  ~ Camels_and_Cactus_Test,
  CIs = TRUE
) +
  labs(
    title = "",
    x = "Phon Output Errors",
    y = "Estimated Fluency Score"
  ) +
  theme_few() +
  theme(
    aspect.ratio = 1.5,
    legend.position = "top"
  )

plot_phon


# Immediate serial recall word span
span_fluency <- emmip(
  emm_results$Immediate_Serial_Recall_Wordspan,
  ~ Immediate_Serial_Recall_Wordspan,
  CIs = TRUE
) +
  labs(
    title = "",
    x = "Immediate Serial Recall Word Span",
    y = "Estimated Fluency Score"
  ) +
  theme_few() +
  theme(
    aspect.ratio = 1.5,
    legend.position = "top"
  )

span_fluency


# Rhyme probe
rhyme_fluency <- emmip(
  emm_results$Rhyme_Probe,
  ~ Rhyme_Probe,
  CIs = TRUE
) +
  labs(
    title = "",
    x = "Rhyme Span",
    y = "Estimated Fluency Score"
  ) +
  theme_few() +
  theme(
    aspect.ratio = 1.5,
    legend.position = "top"
  )

rhyme_fluency



```
## Battery x Lexical plots

```{r}
# camels & cactus and synonymy triplet x phonological similarity

# match prev PWA color
koons_colors <- paletteer_d("MoMAColors::Koons")
second_color <- koons_colors[2]

# plot raw score scatter with model predicted line & CI
overlay_emm_scatter <- function(model, df, pred, y_lab) {

  # numeric covariates to fix at medians
  num_vars <- names(Filter(is.numeric, model.frame(model)))
  num_vars <- setdiff(num_vars, c(pred, "value"))  # don't override pred or outcome

  at_list <- setNames(as.list(rep(NA_real_, length(num_vars))), num_vars)
  for (v in num_vars) at_list[[v]] <- median(model.frame(model)[[v]], na.rm = TRUE)

  emm <- emmeans(
    model,
    as.formula(paste("~", pred)),
    cov.reduce = range,
    at = at_list
  )

  emm_df <- as.data.frame(emm)

  ggplot() +
    geom_point(
      data = df,
      aes(x = .data[[pred]], y = value),
      size = 2, alpha = 0.7, color = second_color
    ) +
    geom_ribbon(
      data = emm_df,
      aes(x = .data[[pred]], ymin = lower.CL, ymax = upper.CL),
      alpha = 0.2, fill = "cyan4"
    ) +
    geom_line(
      data = emm_df,
      aes(x = .data[[pred]], y = emmean),
      linewidth = 1, color = "cyan4"
    ) +
    theme_few() +
    theme(legend.position = "none", aspect.ratio = 1.5,
          axis.text.x = element_text(hjust = 1, face = "bold")) +
    labs(x = pred, y = y_lab)
}

# Filter data/models
df_phon  <- lexical_battery_long %>% filter(cue == "phonological")
res_phon <- lexical_battery_results %>% filter(cue == "phonological")

# get model (keeping your original indexing)
mod <- lexical_battery_results$model[[2]]

# overlay (UPDATED variable names)
camel_overlay <- overlay_emm_scatter(
  model = mod,
  df    = df_phon,
  pred  = "Camels_and_Cactus_Test",
  y_lab = "Estimated Phonological Similarity"
)

syntot_overlay <- overlay_emm_scatter(
  model = mod,
  df    = df_phon,
  pred  = "Synonymy_Triplet_Test",
  y_lab = "Estimated Phonological Similarity"
)

camel_overlay + syntot_overlay



ggplot(
  data = lexical_battery_wide,
  aes(x = Camels_and_Cactus_Test, y = phonological)
) +
  geom_point(size = 2, alpha = 0.7, color = second_color) +
  geom_smooth(method = "lm", se = TRUE)
```


################################################
#Demographics

```{r}
#import demographics
#dx<-dx%>%rename(Subject=ID)
#demos<-read.csv("demographics_deid.csv")%>%filter(Subject %in% all_lexical$Subject)%>%left_join(dx)

# subtypes
#type_summary <- demos %>% filter(Group=="PWA")%>%group_by(Group)%>%
#  count(LangDx) %>%
#  mutate(prop = n / sum(n))
#type_summary

# gender
#gender_summary <- demos %>% group_by(Group)%>%
#  count(Gender) %>%
# mutate(prop = n / sum(n))
#gender_summary

# age
#avg_age <- demos %>%group_by(Group)%>%
# summarise(
#    mean_age = mean(Age, na.rm = TRUE),
#    sd_age   = sd(Age, na.rm = TRUE),
#    min_age  = min(Age, na.rm = TRUE),
#    max_age  = max(Age, na.rm = TRUE))
#avg_age

# education
#edu_summary <- demos %>%group_by(Group)%>%
#  count(Education) %>%
#  mutate(prop = n / sum(n))
#edu_summary

# race
#race_summary <- demos %>%group_by(Group)%>%
#  count(EthnicGrp) %>%
 # mutate(prop = n / sum(n))
#race_summary


# for table
#battery_wide<-battery_long%>%
#    pivot_wider(
#    id_cols = Subject,
#    names_from = test,
#    values_from = value)
#table_df<-battery_wide%>%left_join(demos)%>%unique()
#write.csv(table_df,"table_info.csv")
```







